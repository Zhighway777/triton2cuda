#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„CUDA Kernelæµ‹è¯•æ–‡ä»¶
ä¸“é—¨ç”¨äºæµ‹è¯• vecadd_temp.py ä¸­çš„CUDA kerneléƒ¨åˆ†
"""

import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline
import os
import time
import numpy as np

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["TORCH_USE_CUDA_DSA"] = "1"

class CUDAKernelExtractor:
    """
    CUDA Kernel æå–å™¨å’Œæµ‹è¯•å™¨
    """
    
    def __init__(self):
        self.cuda_module = None
        self.compilation_success = False
        
    def extract_and_compile_kernel(self):
        """
        æå–å¹¶ç¼–è¯‘CUDA kernel
        """
        print("ğŸ”§ æå–å¹¶ç¼–è¯‘CUDA Kernel...")
        
        # æå–çš„çº¯CUDA kernelä»£ç 
        cuda_source = """
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <stdio.h>

// ============ CUDA KERNEL æ ¸å¿ƒéƒ¨åˆ† ============
__global__ void add_kernel(
    const float* x_ptr,        // è¾“å…¥å¼ é‡1çš„æŒ‡é’ˆ
    const float* y_ptr,        // è¾“å…¥å¼ é‡2çš„æŒ‡é’ˆ  
    float* output_ptr,         // è¾“å‡ºå¼ é‡çš„æŒ‡é’ˆ
    int n_elements,            // æ€»å…ƒç´ æ•°é‡
    int BLOCK_SIZE             // çº¿ç¨‹å—å¤§å°
) {
    // 1. è®¡ç®—å½“å‰çº¿ç¨‹çš„å…¨å±€ç´¢å¼•
    int idx = blockIdx.x * BLOCK_SIZE + threadIdx.x;
    
    // 2. è¾¹ç•Œæ£€æŸ¥ï¼Œé˜²æ­¢å†…å­˜è¶Šç•Œè®¿é—®
    if (idx < n_elements) {
        // 3. æ‰§è¡Œå‘é‡åŠ æ³•è¿ç®—
        output_ptr[idx] = x_ptr[idx] + y_ptr[idx];
        
        // å¯é€‰ï¼šè°ƒè¯•ä¿¡æ¯ï¼ˆä»…åœ¨å°æ•°æ®é›†æ—¶å¯ç”¨ï¼‰
        // if (idx < 10) {
        //     printf("Thread %d: %.2f + %.2f = %.2f\\n", 
        //            idx, x_ptr[idx], y_ptr[idx], output_ptr[idx]);
        // }
    }
}

// ============ C++ åŒ…è£…å‡½æ•° ============
torch::Tensor cuda_add_wrapper(torch::Tensor x_tensor, torch::Tensor y_tensor) {
    // 1. è¾“å…¥éªŒè¯
    TORCH_CHECK(x_tensor.is_cuda(), "è¾“å…¥å¼ é‡xå¿…é¡»åœ¨CUDAè®¾å¤‡ä¸Š");
    TORCH_CHECK(y_tensor.is_cuda(), "è¾“å…¥å¼ é‡yå¿…é¡»åœ¨CUDAè®¾å¤‡ä¸Š");
    TORCH_CHECK(x_tensor.is_contiguous(), "è¾“å…¥å¼ é‡xå¿…é¡»æ˜¯è¿ç»­çš„");
    TORCH_CHECK(y_tensor.is_contiguous(), "è¾“å…¥å¼ é‡yå¿…é¡»æ˜¯è¿ç»­çš„");
    TORCH_CHECK(x_tensor.sizes() == y_tensor.sizes(), "è¾“å…¥å¼ é‡å°ºå¯¸å¿…é¡»ç›¸åŒ");
    TORCH_CHECK(x_tensor.dtype() == torch::kFloat32, "ä»…æ”¯æŒfloat32ç±»å‹");
    TORCH_CHECK(y_tensor.dtype() == torch::kFloat32, "ä»…æ”¯æŒfloat32ç±»å‹");

    // 2. è·å–å¼ é‡ä¿¡æ¯
    auto total_size = x_tensor.numel();
    auto output_tensor = torch::empty_like(x_tensor);

    // 3. é…ç½®CUDA kernelå¯åŠ¨å‚æ•°
    const int BLOCK_SIZE = 256;  // æ¯ä¸ªçº¿ç¨‹å—256ä¸ªçº¿ç¨‹
    const int blocks_per_grid = (total_size + BLOCK_SIZE - 1) / BLOCK_SIZE;
    
    // 4. è°ƒè¯•ä¿¡æ¯
    // printf("å¯åŠ¨kernel: blocks=%d, threads_per_block=%d, total_elements=%ld\\n", 
    //        blocks_per_grid, BLOCK_SIZE, total_size);

    // 5. å¯åŠ¨CUDA kernel
    add_kernel<<<blocks_per_grid, BLOCK_SIZE>>>(
        x_tensor.data_ptr<float>(),
        y_tensor.data_ptr<float>(),
        output_tensor.data_ptr<float>(),
        total_size,
        BLOCK_SIZE
    );

    // 6. æ£€æŸ¥kernelå¯åŠ¨é”™è¯¯
    cudaError_t launch_err = cudaGetLastError();
    if (launch_err != cudaSuccess) {
        throw std::runtime_error(
            std::string("CUDA kernelå¯åŠ¨å¤±è´¥: ") + cudaGetErrorString(launch_err)
        );
    }

    // 7. ç­‰å¾…kernelæ‰§è¡Œå®Œæˆ
    cudaError_t sync_err = cudaDeviceSynchronize();
    if (sync_err != cudaSuccess) {
        throw std::runtime_error(
            std::string("CUDA kernelæ‰§è¡Œå¤±è´¥: ") + cudaGetErrorString(sync_err)
        );
    }

    return output_tensor;
}

// ============ é¢å¤–çš„æµ‹è¯•å‡½æ•° ============
void print_device_info() {
    int device;
    cudaGetDevice(&device);
    
    cudaDeviceProp prop;
    cudaGetDeviceProperties(&prop, device);
    
    printf("=== CUDAè®¾å¤‡ä¿¡æ¯ ===\\n");
    printf("è®¾å¤‡åç§°: %s\\n", prop.name);
    printf("è®¡ç®—èƒ½åŠ›: %d.%d\\n", prop.major, prop.minor);
    printf("å¤šå¤„ç†å™¨æ•°é‡: %d\\n", prop.multiProcessorCount);
    printf("æ¯ä¸ªçº¿ç¨‹å—æœ€å¤§çº¿ç¨‹æ•°: %d\\n", prop.maxThreadsPerBlock);
    printf("å…¨å±€å†…å­˜å¤§å°: %.2f GB\\n", prop.totalGlobalMem / 1024.0 / 1024.0 / 1024.0);
    printf("===================\\n");
}

int test_kernel_basic() {
    print_device_info();
    
    // åˆ›å»ºæµ‹è¯•æ•°æ®
    const int N = 1000;
    float *h_x = (float*)malloc(N * sizeof(float));
    float *h_y = (float*)malloc(N * sizeof(float));
    float *h_result = (float*)malloc(N * sizeof(float));
    
    // åˆå§‹åŒ–æ•°æ®
    for (int i = 0; i < N; i++) {
        h_x[i] = i * 1.0f;
        h_y[i] = i * 2.0f;
    }
    
    // åˆ†é…GPUå†…å­˜
    float *d_x, *d_y, *d_result;
    cudaMalloc(&d_x, N * sizeof(float));
    cudaMalloc(&d_y, N * sizeof(float));
    cudaMalloc(&d_result, N * sizeof(float));
    
    // å¤åˆ¶æ•°æ®åˆ°GPU
    cudaMemcpy(d_x, h_x, N * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_y, h_y, N * sizeof(float), cudaMemcpyHostToDevice);
    
    // å¯åŠ¨kernel
    const int BLOCK_SIZE = 256;
    const int blocks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;
    add_kernel<<<blocks, BLOCK_SIZE>>>(d_x, d_y, d_result, N, BLOCK_SIZE);
    
    // æ£€æŸ¥é”™è¯¯
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf("Kernelå¯åŠ¨å¤±è´¥: %s\\n", cudaGetErrorString(err));
        return -1;
    }
    
    // ç­‰å¾…å®Œæˆ
    cudaDeviceSynchronize();
    
    // å¤åˆ¶ç»“æœå›CPU
    cudaMemcpy(h_result, d_result, N * sizeof(float), cudaMemcpyDeviceToHost);
    
    // éªŒè¯ç»“æœ
    bool correct = true;
    for (int i = 0; i < N; i++) {
        float expected = h_x[i] + h_y[i];
        if (abs(h_result[i] - expected) > 1e-5) {
            printf("é”™è¯¯ä½ç½® %d: å¾—åˆ° %.2f, æœŸæœ› %.2f\\n", i, h_result[i], expected);
            correct = false;
            break;
        }
    }
    
    printf("åŸºç¡€kernelæµ‹è¯•: %s\\n", correct ? "é€šè¿‡" : "å¤±è´¥");
    
    // æ¸…ç†å†…å­˜
    free(h_x); free(h_y); free(h_result);
    cudaFree(d_x); cudaFree(d_y); cudaFree(d_result);
    
    return correct ? 0 : -1;
}
"""

        # C++æ¥å£å£°æ˜
        cpp_source = """
torch::Tensor cuda_add_wrapper(torch::Tensor x_tensor, torch::Tensor y_tensor);
int test_kernel_basic();
void print_device_info();
"""

        try:
            print("  ç¼–è¯‘CUDAä»£ç ...")
            self.cuda_module = load_inline(
                name="cuda_kernel_test",
                cpp_sources=cpp_source,
                cuda_sources=cuda_source,
                functions=["cuda_add_wrapper", "test_kernel_basic", "print_device_info"],
                verbose=True,  # æ˜¾ç¤ºç¼–è¯‘è¯¦æƒ…
                extra_cflags=["-O2"],
                extra_cuda_cflags=["-O2", "--use_fast_math", "-arch=sm_75"]  # æ ¹æ®æ‚¨çš„GPUè°ƒæ•´æ¶æ„
            )
            
            print("  âœ… CUDA Kernelç¼–è¯‘æˆåŠŸ!")
            self.compilation_success = True
            return True
            
        except Exception as e:
            print(f"  âŒ ç¼–è¯‘å¤±è´¥: {e}")
            self.compilation_success = False
            return False
    
    def test_compilation(self):
        """
        æµ‹è¯•ç¼–è¯‘æ­£ç¡®æ€§
        """
        print("\nğŸ“ æµ‹è¯•1: ç¼–è¯‘æ­£ç¡®æ€§")
        print("-" * 30)
        
        success = self.extract_and_compile_kernel()
        if success:
            print("âœ… ç¼–è¯‘æµ‹è¯•é€šè¿‡")
        else:
            print("âŒ ç¼–è¯‘æµ‹è¯•å¤±è´¥")
        return success
    
    def test_device_info(self):
        """
        æµ‹è¯•è®¾å¤‡ä¿¡æ¯è·å–
        """
        if not self.compilation_success:
            return False
            
        print("\nğŸ–¥ï¸  è·å–CUDAè®¾å¤‡ä¿¡æ¯")
        print("-" * 30)
        
        try:
            self.cuda_module.print_device_info()
            return True
        except Exception as e:
            print(f"âŒ è·å–è®¾å¤‡ä¿¡æ¯å¤±è´¥: {e}")
            return False
    
    def test_basic_kernel(self):
        """
        æµ‹è¯•åŸºç¡€kernelåŠŸèƒ½
        """
        if not self.compilation_success:
            return False
            
        print("\nğŸ§ª æµ‹è¯•2: åŸºç¡€KernelåŠŸèƒ½")
        print("-" * 30)
        
        try:
            result = self.cuda_module.test_kernel_basic()
            if result == 0:
                print("âœ… åŸºç¡€kernelæµ‹è¯•é€šè¿‡")
                return True
            else:
                print("âŒ åŸºç¡€kernelæµ‹è¯•å¤±è´¥")
                return False
        except Exception as e:
            print(f"âŒ åŸºç¡€kernelæµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def test_pytorch_integration(self):
        """
        æµ‹è¯•PyTorché›†æˆ
        """
        if not self.compilation_success:
            return False
            
        print("\nğŸ”— æµ‹è¯•3: PyTorché›†æˆ")
        print("-" * 30)
        
        test_cases = [
            {"name": "å°å¼ é‡", "size": (100,)},
            {"name": "ä¸­ç­‰å¼ é‡", "size": (10000,)},
            {"name": "å¤§å¼ é‡", "size": (1000000,)},
            {"name": "2Då¼ é‡", "size": (1000, 100)},
            {"name": "3Då¼ é‡", "size": (10, 100, 100)},
        ]
        
        all_passed = True
        
        for case in test_cases:
            try:
                print(f"  æµ‹è¯• {case['name']} {case['size']}...")
                
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                x = torch.randn(case['size'], dtype=torch.float32, device='cuda')
                y = torch.randn(case['size'], dtype=torch.float32, device='cuda')
                
                # ä½¿ç”¨CUDA kernel
                start_time = time.time()
                cuda_result = self.cuda_module.cuda_add_wrapper(x, y)
                cuda_time = time.time() - start_time
                
                # ä½¿ç”¨PyTorchåŸç”Ÿ
                start_time = time.time()
                pytorch_result = x + y
                pytorch_time = time.time() - start_time
                
                # æ£€æŸ¥ç»“æœæ­£ç¡®æ€§
                max_diff = torch.max(torch.abs(cuda_result - pytorch_result)).item()
                
                print(f"    æœ€å¤§å·®å¼‚: {max_diff:.2e}")
                print(f"    CUDAæ—¶é—´: {cuda_time*1000:.3f}ms")
                print(f"    PyTorchæ—¶é—´: {pytorch_time*1000:.3f}ms")
                print(f"    åŠ é€Ÿæ¯”: {pytorch_time/cuda_time:.2f}x")
                
                if max_diff > 1e-5:
                    print(f"    âŒ æ•°å€¼ç²¾åº¦æµ‹è¯•å¤±è´¥")
                    all_passed = False
                else:
                    print(f"    âœ… æµ‹è¯•é€šè¿‡")
                    
            except Exception as e:
                print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
                all_passed = False
        
        return all_passed
    
    def test_error_handling(self):
        """
        æµ‹è¯•é”™è¯¯å¤„ç†
        """
        if not self.compilation_success:
            return False
            
        print("\nâš ï¸  æµ‹è¯•4: é”™è¯¯å¤„ç†")
        print("-" * 30)
        
        error_cases = [
            {
                "name": "CPUå¼ é‡è¾“å…¥",
                "setup": lambda: (torch.randn(100), torch.randn(100, device='cuda')),
                "should_fail": True
            },
            {
                "name": "ä¸åŒå°ºå¯¸å¼ é‡",
                "setup": lambda: (torch.randn(100, device='cuda'), torch.randn(200, device='cuda')),
                "should_fail": True
            },
            {
                "name": "é”™è¯¯æ•°æ®ç±»å‹",
                "setup": lambda: (torch.randn(100, device='cuda', dtype=torch.float64), 
                                torch.randn(100, device='cuda', dtype=torch.float32)),
                "should_fail": True
            },
            {
                "name": "æ­£å¸¸æƒ…å†µ",
                "setup": lambda: (torch.randn(100, device='cuda'), torch.randn(100, device='cuda')),
                "should_fail": False
            }
        ]
        
        all_passed = True
        
        for case in error_cases:
            print(f"  æµ‹è¯• {case['name']}...")
            try:
                x, y = case['setup']()
                result = self.cuda_module.cuda_add_wrapper(x, y)
                
                if case['should_fail']:
                    print(f"    âŒ åº”è¯¥å¤±è´¥ä½†æˆåŠŸäº†")
                    all_passed = False
                else:
                    print(f"    âœ… æŒ‰é¢„æœŸæˆåŠŸ")
                    
            except Exception as e:
                if case['should_fail']:
                    print(f"    âœ… æŒ‰é¢„æœŸå¤±è´¥: {str(e)[:50]}...")
                else:
                    print(f"    âŒ ä¸åº”è¯¥å¤±è´¥: {e}")
                    all_passed = False
        
        return all_passed
    
    def run_all_tests(self):
        """
        è¿è¡Œæ‰€æœ‰æµ‹è¯•
        """
        print("ğŸš€ å¼€å§‹CUDA Kernelç‹¬ç«‹æµ‹è¯•")
        print("="*50)
        
        tests = [
            ("ç¼–è¯‘æ­£ç¡®æ€§", self.test_compilation),
            ("è®¾å¤‡ä¿¡æ¯", self.test_device_info),
            ("åŸºç¡€Kernel", self.test_basic_kernel),
            ("PyTorché›†æˆ", self.test_pytorch_integration),
            ("é”™è¯¯å¤„ç†", self.test_error_handling),
        ]
        
        results = []
        
        for test_name, test_func in tests:
            try:
                result = test_func()
                results.append((test_name, result))
            except Exception as e:
                print(f"âŒ {test_name}æµ‹è¯•å¼‚å¸¸: {e}")
                results.append((test_name, False))
        
        # è¾“å‡ºæ€»ç»“
        print("\n" + "="*50)
        print("ğŸ“Š æµ‹è¯•æ€»ç»“")
        print("="*50)
        
        for test_name, passed in results:
            status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
            print(f"{test_name}: {status}")
        
        overall_success = all(result[1] for result in results)
        print(f"\næ€»ä½“ç»“æœ: {'ğŸ‰ å…¨éƒ¨é€šè¿‡' if overall_success else 'ğŸ’¥ å­˜åœ¨é—®é¢˜'}")
        
        return overall_success

def main():
    """
    ä¸»å‡½æ•°
    """
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥CUDAå®‰è£…")
        return
    
    print(f"âœ… CUDAå¯ç”¨ï¼Œè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
    print(f"å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
    print(f"è®¾å¤‡åç§°: {torch.cuda.get_device_name()}")
    print()
    
    tester = CUDAKernelExtractor()
    success = tester.run_all_tests()
    
    if success:
        print("\nğŸ¯ å»ºè®®: CUDA Kernelå·¥ä½œæ­£å¸¸ï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨")
    else:
        print("\nğŸ”§ å»ºè®®: è¯·æ£€æŸ¥CUDAå®‰è£…å’ŒGPUé©±åŠ¨")

if __name__ == "__main__":
    main() 