import os
import sys
import tempfile
import torch
import importlib.util
import sys
import glob
import argparse
import requests
import traceback

# ä¿®å¤è·¯å¾„é—®é¢˜ï¼šç¡®ä¿è·¯å¾„å§‹ç»ˆæ­£ç¡®
current_file_dir = os.path.dirname(os.path.abspath(__file__))  # evalç›®å½•
folder_path = os.path.dirname(current_file_dir)  # solutionç›®å½•

# ç¡®ä¿å·¥ä½œç›®å½•æ­£ç¡®
original_cwd = os.getcwd()
os.chdir(folder_path)

sys.path.append(folder_path)
sys.path.append(os.path.join(folder_path, "example_submission"))
sys.path.append(os.path.join(folder_path, "data", "ref"))

TEST_NN_MODEL_NAME = 'ModelNew'

from tri2cu import triton2cuda

class NetworkMonitor:
    """APIè¿é€šæ€§ç›‘æ§ç±»"""
    
    @staticmethod
    def check_api_endpoints():
        """æ£€æŸ¥APIç«¯ç‚¹å¯ç”¨æ€§"""
        endpoints = {
            "æ™ºè°±AI": "https://open.bigmodel.cn/api/paas/v4/chat/completions",
            "OpenRouter": "https://openrouter.ai/api/v1/chat/completions",
            "DeepSeek": "https://api.deepseek.com/v1/chat/completions"
        }
        
        results = {}
        for name, url in endpoints.items():
            try:
                # åªæ£€æŸ¥ç«¯ç‚¹æ˜¯å¦å¯è¾¾ï¼Œä¸å‘é€å®é™…è¯·æ±‚
                response = requests.head(url, timeout=10)
                results[name] = {
                    "available": response.status_code != 404,
                    "status_code": response.status_code,
                    "response_time": response.elapsed.total_seconds()
                }
            except Exception as e:
                results[name] = {
                    "available": False,
                    "error": str(e)
                }
        
        return results

def classify_api_error(error_message):
    """åˆ†ç±»APIé”™è¯¯ç±»å‹"""
    error_msg_lower = error_message.lower()
    
    # APIè¿æ¥è¶…æ—¶
    if any(keyword in error_msg_lower for keyword in ["timeout", "timed out"]):
        return "TimeoutError"
    
    # è¿æ¥é”™è¯¯
    if any(keyword in error_msg_lower for keyword in ["connection", "unreachable", "dns"]):
        return "ConnectionError"
    
    # APIè®¤è¯é”™è¯¯
    if any(keyword in error_msg_lower for keyword in ["unauthorized", "authentication", "api key", "forbidden"]):
        return "AuthenticationError"
    
    # é€Ÿç‡é™åˆ¶
    if any(keyword in error_msg_lower for keyword in ["rate limit", "quota", "too many requests"]):
        return "RateLimitError"
    
    # APIæœåŠ¡å™¨é”™è¯¯
    if any(keyword in error_msg_lower for keyword in ["500", "502", "503", "504", "server error"]):
        return "ServerError"
    
    # SSL/TLSé”™è¯¯
    if any(keyword in error_msg_lower for keyword in ["ssl", "certificate", "tls"]):
        return "SSLError"
    
    # ä¸€èˆ¬APIè¿æ¥é”™è¯¯
    if any(keyword in error_msg_lower for keyword in ["network", "socket", "http"]):
        return "NetworkError"
    
    return "UnknownError"

def test_api_connectivity():
    """æµ‹è¯•APIç«¯ç‚¹å¯ç”¨æ€§"""
    print("=== APIè¿é€šæ€§æµ‹è¯• ===")
    
    # APIç«¯ç‚¹æµ‹è¯•
    print("APIç«¯ç‚¹æµ‹è¯•:")
    api_status = NetworkMonitor.check_api_endpoints()
    
    for name, status in api_status.items():
        if status.get("available", False):
            response_time = status.get("response_time", 0)
            print(f"  {name}: âœ“ å¯ç”¨ (å“åº”æ—¶é—´: {response_time:.2f}s)")
        else:
            error = status.get("error", "ä¸å¯ç”¨")
            print(f"  {name}: âœ— ä¸å¯ç”¨ ({error})")
    
    available_apis = [name for name, status in api_status.items() if status.get("available", False)]
    
    if available_apis:
        print(f"\nå¯ç”¨API: {', '.join(available_apis)}")
        return True
    else:
        print("\nâŒ æ‰€æœ‰APIéƒ½ä¸å¯ç”¨")
        return False

def check_file_exists(file_path, description="æ–‡ä»¶"):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æä¾›è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯"""
    if os.path.exists(file_path):
        return True
    
    print(f"âŒ {description}ä¸å­˜åœ¨: {file_path}")
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"ç»å¯¹è·¯å¾„: {os.path.abspath(file_path)}")
    
    # æ£€æŸ¥çˆ¶ç›®å½•æ˜¯å¦å­˜åœ¨
    parent_dir = os.path.dirname(file_path)
    if os.path.exists(parent_dir):
        print(f"çˆ¶ç›®å½•å­˜åœ¨ï¼ŒåŒ…å«æ–‡ä»¶:")
        try:
            files = os.listdir(parent_dir)
            for f in files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
                print(f"  - {f}")
            if len(files) > 10:
                print(f"  ... è¿˜æœ‰ {len(files) - 10} ä¸ªæ–‡ä»¶")
        except Exception as e:
            print(f"  æ— æ³•åˆ—å‡ºçˆ¶ç›®å½•å†…å®¹: {e}")
    else:
        print(f"çˆ¶ç›®å½•ä¹Ÿä¸å­˜åœ¨: {parent_dir}")
    
    return False

def get_inputs_for_file(file_name):
    """ä¸ºä¸åŒçš„æ–‡ä»¶æä¾›ç›¸åº”çš„è¾“å…¥æ•°æ®ï¼ˆå½“æ²¡æœ‰å‚è€ƒå®ç°æ—¶ä½¿ç”¨ï¼‰"""
    if file_name == "vecadd.py":
        # ä¸¤ä¸ªç›¸åŒå½¢çŠ¶çš„å¼ é‡
        return [
            [torch.randn(1024, device='cuda', dtype=torch.float32), torch.randn(1024, device='cuda', dtype=torch.float32)],
            [torch.randn(8432, device='cuda', dtype=torch.float32), torch.randn(8432, device='cuda', dtype=torch.float32)]
        ]
    elif file_name in ["constant_add.py", "constant_add_block.py"]:
        # ä¸€ä¸ªå¼ é‡
        return [
            [torch.randn(1024, device='cuda', dtype=torch.float32)],
            [torch.randn(200, device='cuda', dtype=torch.float32)]
        ]
    elif file_name in ["outer_vecadd.py", "outer_vecadd_block.py"]:
        # ä¸¤ä¸ª1Då¼ é‡
        return [
            [torch.randn(32, device='cuda', dtype=torch.float32), torch.randn(32, device='cuda', dtype=torch.float32)],
            [torch.randn(100, device='cuda', dtype=torch.float32), torch.randn(90, device='cuda', dtype=torch.float32)]
        ]
    elif file_name in ["longsum.py", "longsoftmax.py", "softmax.py"]:
        # ä¸€ä¸ª2Då¼ é‡
        return [
            [torch.randn(4, 200, device='cuda', dtype=torch.float32)],
            [torch.randn(8, 128, device='cuda', dtype=torch.float32)]
        ]
    elif file_name == "conv2d.py":
        # è¾“å…¥å¼ é‡å’Œå·ç§¯æ ¸
        return [
            [torch.randn(4, 8, 8, device='cuda', dtype=torch.float32), torch.randn(4, 4, device='cuda', dtype=torch.float32)],
            [torch.randn(2, 6, 6, device='cuda', dtype=torch.float32), torch.randn(3, 3, device='cuda', dtype=torch.float32)]
        ]
    elif file_name == "flashatt.py":
        # q, k, vä¸‰ä¸ªå¼ é‡
        return [
            [torch.randn(200, device='cuda', dtype=torch.float32), torch.randn(200, device='cuda', dtype=torch.float32), torch.randn(200, device='cuda', dtype=torch.float32)],
            [torch.randn(128, device='cuda', dtype=torch.float32), torch.randn(128, device='cuda', dtype=torch.float32), torch.randn(128, device='cuda', dtype=torch.float32)]
        ]
    elif file_name == "matmul.py":
        # ä¸¤ä¸ª3Då¼ é‡ï¼ˆæ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼‰
        return [
            [torch.randn(4, 32, 32, device='cuda', dtype=torch.float32), torch.randn(4, 32, 32, device='cuda', dtype=torch.float32)],
            [torch.randn(2, 16, 24, device='cuda', dtype=torch.float32), torch.randn(2, 24, 20, device='cuda', dtype=torch.float32)]
        ]
    elif file_name == "mul_relu_fused_block.py":
        # ä¸¤ä¸ª1Då¼ é‡
        return [
            [torch.randn(100, device='cuda', dtype=torch.float32), torch.randn(90, device='cuda', dtype=torch.float32)],
            [torch.randn(64, device='cuda', dtype=torch.float32), torch.randn(48, device='cuda', dtype=torch.float32)]
        ]
    elif file_name == "outer_mul_relu_fused_block.py":
        # ä¸‰ä¸ªå¼ é‡ï¼ˆx, y, dzï¼‰
        return [
            [torch.randn(90, 100, device='cuda', dtype=torch.float32), torch.randn(90, device='cuda', dtype=torch.float32), torch.randn(90, 100, device='cuda', dtype=torch.float32)],
            [torch.randn(64, 48, device='cuda', dtype=torch.float32), torch.randn(64, device='cuda', dtype=torch.float32), torch.randn(64, 48, device='cuda', dtype=torch.float32)]
        ]
    elif file_name == "quant_matmul.py":
        # å››ä¸ªå¼ é‡ï¼ˆscale, offset, weight, activationï¼‰
        return [
            [
                torch.randn(32, 8, device='cuda', dtype=torch.float32),
                torch.randint(0, 15, (32,), device='cuda', dtype=torch.int32),
                torch.randint(0, 15, (32, 8), device='cuda', dtype=torch.int32),
                torch.randn(64, 32, device='cuda', dtype=torch.float32)
            ],
            [
                torch.randn(16, 8, device='cuda', dtype=torch.float32),
                torch.randint(0, 15, (16,), device='cuda', dtype=torch.int32),
                torch.randint(0, 15, (16, 8), device='cuda', dtype=torch.int32),
                torch.randn(64, 16, device='cuda', dtype=torch.float32)
            ]
        ]
    elif file_name == "02-fused-softmax.py":
        # ä¸€ä¸ª2Då¼ é‡ (n_rows, n_cols)
        return [
            [torch.randn(32, 128, device='cuda', dtype=torch.float32)],
            [torch.randn(64, 256, device='cuda', dtype=torch.float32)]
        ]
    elif file_name == "03-matrix-multiplication.py":
        # ä¸¤ä¸ª2Då¼ é‡ a (M, K) å’Œ b (K, N)
        return [
            [torch.randn(128, 64, device='cuda', dtype=torch.float32), torch.randn(64, 32, device='cuda', dtype=torch.float32)],
            [torch.randn(256, 128, device='cuda', dtype=torch.float32), torch.randn(128, 64, device='cuda', dtype=torch.float32)]
        ]
    elif file_name == "04-low-memory-dropout.py":
        # ä¸€ä¸ªå¼ é‡ xã€ä¸€ä¸ªfloatå€¼ p å’Œä¸€ä¸ªintå€¼ seed
        return [
            [torch.randn(1024, device='cuda', dtype=torch.float32), 0.1, 42],
            [torch.randn(2048, device='cuda', dtype=torch.float32), 0.2, 123]
        ]
    elif file_name == "05-layer-norm.py":
        # ä¸‰ä¸ªå¼ é‡ xã€weightã€biasï¼Œå’Œä¸€ä¸ªfloatå€¼ eps
        return [
            [torch.randn(32, 256, device='cuda', dtype=torch.float32), torch.randn(256, device='cuda', dtype=torch.float32), torch.randn(256, device='cuda', dtype=torch.float32), 1e-5],
            [torch.randn(64, 512, device='cuda', dtype=torch.float32), torch.randn(512, device='cuda', dtype=torch.float32), torch.randn(512, device='cuda', dtype=torch.float32), 1e-5]
        ]
    elif file_name == "06-fused-attention.py":
        # ä¸‰ä¸ª4Då¼ é‡ q, k, v (BATCH, N_HEAD, N_CTX, HEAD_DIM)
        return [
            [torch.randn(2, 8, 128, 64, device='cuda', dtype=torch.float16), torch.randn(2, 8, 128, 64, device='cuda', dtype=torch.float16), torch.randn(2, 8, 128, 64, device='cuda', dtype=torch.float16), True, 1.0],
            [torch.randn(4, 16, 256, 32, device='cuda', dtype=torch.float16), torch.randn(4, 16, 256, 32, device='cuda', dtype=torch.float16), torch.randn(4, 16, 256, 32, device='cuda', dtype=torch.float16), True, 1.0]
        ]
    elif file_name == "08-grouped-gemm.py":
        # ä¸¤ä¸ªåˆ—è¡¨ group_A å’Œ group_Bï¼Œæ¯ä¸ªåŒ…å«å¤šä¸ª2Då¼ é‡
        return [
            [
                [torch.randn(32, 64, device='cuda', dtype=torch.float16), torch.randn(64, 32, device='cuda', dtype=torch.float16), torch.randn(48, 96, device='cuda', dtype=torch.float16)],
                [torch.randn(64, 32, device='cuda', dtype=torch.float16), torch.randn(32, 48, device='cuda', dtype=torch.float16), torch.randn(96, 64, device='cuda', dtype=torch.float16)]
            ],
            [
                [torch.randn(16, 32, device='cuda', dtype=torch.float16), torch.randn(24, 48, device='cuda', dtype=torch.float16)],
                [torch.randn(32, 16, device='cuda', dtype=torch.float16), torch.randn(48, 24, device='cuda', dtype=torch.float16)]
            ]
        ]
    elif file_name == "09-persistent-matmul.py":
        # ä¸¤ä¸ª2Då¼ é‡ a (M, K) å’Œ b (K, N)
        return [
            [torch.randn(128, 64, device='cuda', dtype=torch.float16), torch.randn(64, 32, device='cuda', dtype=torch.float16)],
            [torch.randn(256, 128, device='cuda', dtype=torch.float16), torch.randn(128, 64, device='cuda', dtype=torch.float16)]
        ]
    elif file_name == "10-block-scaled-matmul.py":
        # å››ä¸ªå¼ é‡ a, b, a_scale, b_scale
        return [
            [
                torch.randn(128, 64, device='cuda', dtype=torch.float16),
                torch.randn(64, 32, device='cuda', dtype=torch.float16),
                torch.randn(32, 8, device='cuda', dtype=torch.float32),
                torch.randn(8, 8, device='cuda', dtype=torch.float32)
            ],
            [
                torch.randn(256, 128, device='cuda', dtype=torch.float16),
                torch.randn(128, 64, device='cuda', dtype=torch.float16),
                torch.randn(64, 16, device='cuda', dtype=torch.float32),
                torch.randn(16, 16, device='cuda', dtype=torch.float32)
            ]
        ]
    else:
        # é»˜è®¤è¿”å›ä¸€ä¸ªå¼ é‡
        return [
            [torch.randn(1024, device='cuda', dtype=torch.float32)],
            [torch.randn(512, device='cuda', dtype=torch.float32)]
        ]

def get_reference_model_and_inputs(file_name):
    """è·å–å‚è€ƒæ¨¡å‹å’Œè¾“å…¥æ•°æ®"""
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„å‚è€ƒå®ç°
    ref_file_path = os.path.join("data", "ref", file_name)
    
    if check_file_exists(ref_file_path, f"å‚è€ƒå®ç° {file_name}"):
        # ä½¿ç”¨å‚è€ƒå®ç°
        print(f"  ä½¿ç”¨å‚è€ƒå®ç°: data/ref/{file_name}")
        module_name = file_name.replace('.py', '_ref')
        spec = importlib.util.spec_from_file_location(module_name, ref_file_path)
        ref_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(ref_module)
        
        # è·å–å‚è€ƒModelç±»å’Œget_inputså‡½æ•°
        RefModel = getattr(ref_module, 'Model', None)
        get_inputs_func = getattr(ref_module, 'get_inputs', None)
        
        if RefModel is None:
            raise ValueError(f"æ— æ³•åœ¨å‚è€ƒå®ç° {file_name} ä¸­æ‰¾åˆ° Model ç±»")
        
        if get_inputs_func is None:
            print(f"  è­¦å‘Š: å‚è€ƒå®ç° {file_name} ä¸­æ²¡æœ‰ get_inputs å‡½æ•°ï¼Œä½¿ç”¨é»˜è®¤è¾“å…¥")
            input_tensors = get_inputs_for_file(file_name)
        else:
            input_tensors = get_inputs_func()
        
        return RefModel, input_tensors
    else:
        # ä½¿ç”¨local_test_listä¸­çš„æ–‡ä»¶ä½œä¸ºå‚è€ƒ
        triton_file_path = os.path.join("data", "triton", "local_test_list", file_name)
        
        if not check_file_exists(triton_file_path, f"Tritonæ–‡ä»¶ {file_name}"):
            raise FileNotFoundError(f"æ— æ³•æ‰¾åˆ°Tritonæ–‡ä»¶: {triton_file_path}")
        
        print(f"  ä½¿ç”¨æœ¬åœ°æµ‹è¯•æ–‡ä»¶ä½œä¸ºå‚è€ƒ: {triton_file_path}")
        
        module_name = file_name.replace('.py', '_triton')
        spec = importlib.util.spec_from_file_location(module_name, triton_file_path)
        triton_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(triton_module)
        
        # è·å–Triton Modelç±»
        TritonModel = getattr(triton_module, 'Model', None)
        if TritonModel is None:
            raise ValueError(f"æ— æ³•åœ¨ {file_name} ä¸­æ‰¾åˆ° Model ç±»")
        
        # ä½¿ç”¨é»˜è®¤è¾“å…¥ç”Ÿæˆå‡½æ•°
        input_tensors = get_inputs_for_file(file_name)
        
        return TritonModel, input_tensors

def save_debug_info(file_name, cuda_code, error_info):
    """ä¿å­˜è°ƒè¯•ä¿¡æ¯åˆ°æ–‡ä»¶"""
    debug_dir = os.path.join(folder_path, "debug_output")
    os.makedirs(debug_dir, exist_ok=True)
    
    # ä¿å­˜è½¬æ¢åçš„CUDAä»£ç 
    cuda_file = os.path.join(debug_dir, f"{file_name}_cuda.py")
    with open(cuda_file, "w") as f:
        f.write(cuda_code)
    
    # ä¿å­˜é”™è¯¯ä¿¡æ¯
    error_file = os.path.join(debug_dir, f"{file_name}_error.txt")
    with open(error_file, "w") as f:
        f.write(error_info)
    
    print(f"  è°ƒè¯•ä¿¡æ¯å·²ä¿å­˜åˆ°: {debug_dir}/")

def eval_single_file(file_name, verbose=False):
    """è¯„æµ‹å•ä¸ªæ–‡ä»¶çš„Tritonåˆ°CUDAè½¬æ¢"""
    print(f"\n=== è¯„æµ‹ {file_name} ===")
    error_messages = []
    
    try:
        # 1. è¯»å–Tritonä»£ç 
        triton_file_path = os.path.join("data", "triton", "local_test_list", file_name)
        
        if not check_file_exists(triton_file_path, f"Tritonæ–‡ä»¶ {file_name}"):
            error_msg = f"æ— æ³•æ‰¾åˆ°Tritonæ–‡ä»¶: {triton_file_path}"
            error_messages.append(error_msg)
            return False, error_messages
        
        with open(triton_file_path, "r") as f:
            triton_code = f.read()
        
        # 2. è½¬æ¢ä¸ºCUDAä»£ç 
        print("  æ­£åœ¨è½¬æ¢Tritonä»£ç ä¸ºCUDAä»£ç ...")
        try:
            cuda_code = triton2cuda(triton_code)
            if verbose:
                print(f"  è½¬æ¢æˆåŠŸï¼Œä»£ç é•¿åº¦: {len(cuda_code)} å­—ç¬¦")
        except Exception as e:
            error_msg = f"è½¬æ¢å¤±è´¥: {str(e)}"
            error_messages.append(error_msg)
            
            # åˆ†ç±»APIé”™è¯¯ç±»å‹
            api_error_type = classify_api_error(str(e))
            if api_error_type in ["TimeoutError", "ConnectionError", "AuthenticationError", "NetworkError"]:
                print(f"  APIé”™è¯¯ [{api_error_type}]: {error_msg}")
                if verbose:
                    print("  ğŸ’¡ å»ºè®®: è¿è¡Œ --network å‚æ•°æµ‹è¯•APIç«¯ç‚¹å¯ç”¨æ€§")
            else:
                print(f"  é”™è¯¯: {error_msg}")
            
            return False, error_messages
        
        # 3. è·å–å‚è€ƒæ¨¡å‹å’Œè¾“å…¥æ•°æ®
        try:
            RefModel, input_tensors = get_reference_model_and_inputs(file_name)
            print(f"  å‡†å¤‡äº† {len(input_tensors)} ç»„æµ‹è¯•æ•°æ®")
        except Exception as e:
            error_msg = f"è·å–å‚è€ƒæ¨¡å‹å¤±è´¥: {str(e)}"
            error_messages.append(error_msg)
            print(f"  é”™è¯¯: {error_msg}")
            return False, error_messages
        
        # 4. åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¹¶åŠ è½½è½¬æ¢åçš„CUDAæ¨¡å—
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_file = os.path.join(temp_dir, "cuda_code.py")
            with open(temp_file, "w") as f:
                f.write(cuda_code)
            
            # åŠ¨æ€åŠ è½½è½¬æ¢åçš„CUDAæ¨¡å—
            spec = importlib.util.spec_from_file_location("cuda_module", temp_file)
            cuda_module = importlib.util.module_from_spec(spec)
            
            # ä¸´æ—¶æ·»åŠ åˆ°sys.modulesä»¥æ”¯æŒç›¸å¯¹å¯¼å…¥
            sys.modules["cuda_module"] = cuda_module
            
            try:
                spec.loader.exec_module(cuda_module)
                print("  CUDAæ¨¡å—åŠ è½½æˆåŠŸ")
            except Exception as e:
                error_msg = f"åŠ è½½è½¬æ¢åçš„CUDAä»£ç å¤±è´¥: {str(e)}"
                error_messages.append(error_msg)
                print(f"  é”™è¯¯: {error_msg}")
                if verbose:
                    import traceback
                    traceback.print_exc()
                    save_debug_info(file_name, cuda_code, f"{error_msg}\n\n{traceback.format_exc()}")
                return False, error_messages
            
            # è·å–è½¬æ¢åçš„ModelNewç±»
            CudaModel = getattr(cuda_module, TEST_NN_MODEL_NAME, None)
            if CudaModel is None:
                error_msg = f"æ— æ³•åœ¨è½¬æ¢åçš„ä»£ç ä¸­æ‰¾åˆ° {TEST_NN_MODEL_NAME} ç±»"
                error_messages.append(error_msg)
                print(f"  é”™è¯¯: {error_msg}")
                if verbose:
                    available_attrs = [attr for attr in dir(cuda_module) if not attr.startswith('_')]
                    print(f"  å¯ç”¨çš„ç±»/å‡½æ•°: {available_attrs}")
                    save_debug_info(file_name, cuda_code, f"{error_msg}\nå¯ç”¨å±æ€§: {available_attrs}")
                return False, error_messages
        
        # 5. å¯¹æ¯”æµ‹è¯•
        success_count = 0
        total_count = len(input_tensors)
        
        for i, input_tensor in enumerate(input_tensors):
            try:
                # ç¡®ä¿è¾“å…¥åœ¨CUDAä¸Š
                input_tensor_cuda = []
                for inp in input_tensor:
                    if not inp.is_cuda:
                        inp = inp.cuda()
                    input_tensor_cuda.append(inp.detach().clone())
                
                # è®¡ç®—å‚è€ƒè¾“å‡º
                try:
                    ref_output = RefModel()(*input_tensor_cuda)
                except Exception as e:
                    error_msg = f"å‚è€ƒæ¨¡å‹è®¡ç®—å¤±è´¥: {str(e)}"
                    print(f"  æµ‹è¯• {i+1}: å¤±è´¥ - {error_msg}")
                    error_messages.append(f"æµ‹è¯• {i+1}: {error_msg}")
                    continue
                
                # è®¡ç®—è½¬æ¢åçš„CUDAè¾“å‡º
                try:
                    cuda_output = CudaModel()(*input_tensor_cuda)
                except Exception as e:
                    error_msg = f"CUDAæ¨¡å‹è®¡ç®—å¤±è´¥: {str(e)}"
                    print(f"  æµ‹è¯• {i+1}: å¤±è´¥ - {error_msg}")
                    error_messages.append(f"æµ‹è¯• {i+1}: {error_msg}")
                    if verbose:
                        import traceback
                        traceback.print_exc()
                    continue
                
                # æ¯”è¾ƒç»“æœ
                try:
                    if torch.allclose(cuda_output, ref_output, atol=1e-3, rtol=1e-3):
                        print(f"  æµ‹è¯• {i+1}: é€šè¿‡")
                        success_count += 1
                    else:
                        error_msg = f"è¾“å‡ºä¸åŒ¹é… - å‚è€ƒè¾“å‡ºå½¢çŠ¶: {ref_output.shape}, CUDAè¾“å‡ºå½¢çŠ¶: {cuda_output.shape}"
                        max_diff = torch.max(torch.abs(cuda_output - ref_output)).item()
                        error_msg += f", æœ€å¤§å·®å¼‚: {max_diff}"
                        print(f"  æµ‹è¯• {i+1}: å¤±è´¥ - {error_msg}")
                        error_messages.append(f"æµ‹è¯• {i+1}: {error_msg}")
                        
                        if verbose:
                            print(f"    å‚è€ƒè¾“å‡ºç»Ÿè®¡: min={ref_output.min().item():.6f}, max={ref_output.max().item():.6f}, mean={ref_output.mean().item():.6f}")
                            print(f"    CUDAè¾“å‡ºç»Ÿè®¡: min={cuda_output.min().item():.6f}, max={cuda_output.max().item():.6f}, mean={cuda_output.mean().item():.6f}")
                            
                except Exception as e:
                    error_msg = f"ç»“æœæ¯”è¾ƒå¤±è´¥: {str(e)}"
                    print(f"  æµ‹è¯• {i+1}: å¤±è´¥ - {error_msg}")
                    error_messages.append(f"æµ‹è¯• {i+1}: {error_msg}")
                    
            except Exception as e:
                error_msg = f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}"
                print(f"  æµ‹è¯• {i+1}: å¤±è´¥ - {error_msg}")
                error_messages.append(f"æµ‹è¯• {i+1}: {error_msg}")
                if verbose:
                    import traceback
                    traceback.print_exc()
        
        print(f"ç»“æœ: {success_count}/{total_count} æµ‹è¯•é€šè¿‡")
        return success_count == total_count, error_messages
        
    except Exception as e:
        error_msg = f"è¯„æµ‹è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {str(e)}"
        error_messages.append(error_msg)
        print(f"é”™è¯¯: {error_msg}")
        if verbose:
            import traceback
            traceback.print_exc()
        return False, error_messages

def eval_all_files(verbose=False):
    """è¯„æµ‹æ‰€æœ‰æ–‡ä»¶"""
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"Solutionç›®å½•: {folder_path}")
    
    # è·å–æ‰€æœ‰tritonæ–‡ä»¶
    test_dir = os.path.join("data", "triton", "local_test_list")
    
    if not check_file_exists(test_dir, "æµ‹è¯•ç›®å½•"):
        print("âŒ æ— æ³•æ‰¾åˆ°æµ‹è¯•ç›®å½•ï¼Œç»ˆæ­¢è¯„æµ‹")
        return False
    
    try:
        triton_files = [f for f in os.listdir(test_dir) if f.endswith('.py')]
        triton_files.sort()  # æŒ‰å­—æ¯é¡ºåºæ’åº
    except Exception as e:
        print(f"âŒ æ— æ³•åˆ—å‡ºæµ‹è¯•ç›®å½•å†…å®¹: {e}")
        return False
    
    print(f"æ‰¾åˆ° {len(triton_files)} ä¸ªtritonæ–‡ä»¶éœ€è¦è¯„æµ‹")
    
    success_files = []
    failed_files = []
    all_errors = {}
    
    for file_name in triton_files:
        success, error_messages = eval_single_file(file_name, verbose)
        if success:
            success_files.append(file_name)
        else:
            failed_files.append(file_name)
            all_errors[file_name] = error_messages
    
    print(f"\n=== æ€»ä½“ç»“æœ ===")
    print(f"æˆåŠŸ: {len(success_files)}/{len(triton_files)} ä¸ªæ–‡ä»¶")
    
    if success_files:
        print("\næˆåŠŸçš„æ–‡ä»¶:")
        for file_name in success_files:
            print(f"  âœ“ {file_name}")
    
    if failed_files:
        print("\nå¤±è´¥çš„æ–‡ä»¶:")
        api_error_files = []
        for file_name in failed_files:
            print(f"  âœ— {file_name}")
            if verbose and file_name in all_errors:
                for error in all_errors[file_name][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                    print(f"    - {error}")
                if len(all_errors[file_name]) > 3:
                    print(f"    - ... è¿˜æœ‰ {len(all_errors[file_name]) - 3} ä¸ªé”™è¯¯")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰APIç›¸å…³é”™è¯¯
            if file_name in all_errors:
                for error in all_errors[file_name]:
                    error_type = classify_api_error(error)
                    if error_type in ["TimeoutError", "ConnectionError", "AuthenticationError", "NetworkError"]:
                        api_error_files.append(file_name)
                        break
        
        # æä¾›APIç›¸å…³é”™è¯¯å»ºè®®
        if api_error_files:
            print(f"\nâš ï¸ æ£€æµ‹åˆ° {len(api_error_files)} ä¸ªæ–‡ä»¶å¯èƒ½å­˜åœ¨APIç›¸å…³é—®é¢˜:")
            for file_name in api_error_files:
                print(f"  - {file_name}")
            print("\nğŸ’¡ APIç›¸å…³é”™è¯¯è§£å†³å»ºè®®:")
            print("1. è¿è¡Œ --network å‚æ•°æµ‹è¯•APIç«¯ç‚¹å¯ç”¨æ€§")
            print("2. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®")
            print("3. ç¡®è®¤APIæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
            print("4. å°è¯•åˆ‡æ¢ç½‘ç»œç¯å¢ƒæˆ–ä½¿ç”¨ä»£ç†")
    
    return len(failed_files) == 0

def eval_simple():
    """åŸå§‹çš„vecaddè¯„æµ‹å‡½æ•°ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
    success, errors = eval_single_file("vecadd.py")
    return success

def eval_golden():
    """ä½¿ç”¨goldenæ ‡å‡†è¯„æµ‹vecadd - æ¯”è¾ƒè½¬æ¢åçš„CUDAä»£ç ä¸golden CUDAå®ç°"""
    print("\n=== Goldenæ ‡å‡†è¯„æµ‹ ===")
    print("æ¯”è¾ƒ: è½¬æ¢åçš„CUDAä»£ç  vs Golden CUDAå®ç°")
    
    try:
        # 1. è¯»å–Tritonä»£ç å¹¶è½¬æ¢ä¸ºCUDA
        triton_file_path = os.path.join("data", "triton", "local_test_list", "vecadd.py")
        
        if not check_file_exists(triton_file_path, "Triton vecaddæ–‡ä»¶"):
            return False
        
        with open(triton_file_path, "r") as f:
            triton_code = f.read()
        
        cuda_code = triton2cuda(triton_code)
        print("âœ“ Tritonä»£ç è½¬æ¢å®Œæˆ")
        
        # 2. åŠ è½½å‚è€ƒå®ç°è·å–è¾“å…¥æ•°æ®
        from vecadd import Model as RefModel, get_inputs
        input_tensors = get_inputs()
        print(f"âœ“ è·å–äº† {len(input_tensors)} ç»„æµ‹è¯•æ•°æ®")
        
        # 3. åŠ è½½golden CUDAå®ç°
        golden_file_path = os.path.join("data", "cuda", "vecadd.py")
        
        if not check_file_exists(golden_file_path, "Golden CUDAæ–‡ä»¶"):
            return False
        
        spec = importlib.util.spec_from_file_location("golden_cuda", golden_file_path)
        golden_module = importlib.util.module_from_spec(spec)
        sys.modules["golden_cuda"] = golden_module
        spec.loader.exec_module(golden_module)
        
        GoldenModel = getattr(golden_module, TEST_NN_MODEL_NAME, None)
        if GoldenModel is None:
            raise ValueError(f"æ— æ³•åœ¨goldenå®ç°ä¸­æ‰¾åˆ° {TEST_NN_MODEL_NAME} ç±»")
        print("âœ“ Golden CUDAå®ç°åŠ è½½å®Œæˆ")
        
        # 4. åŠ è½½è½¬æ¢åçš„CUDAä»£ç 
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_file = os.path.join(temp_dir, "converted_cuda.py")
            with open(temp_file, "w") as f:
                f.write(cuda_code)
            
            spec = importlib.util.spec_from_file_location("converted_cuda", temp_file)
            converted_module = importlib.util.module_from_spec(spec)
            sys.modules["converted_cuda"] = converted_module
            spec.loader.exec_module(converted_module)
            
            ConvertedModel = getattr(converted_module, TEST_NN_MODEL_NAME, None)
            if ConvertedModel is None:
                raise ValueError(f"æ— æ³•åœ¨è½¬æ¢åçš„ä»£ç ä¸­æ‰¾åˆ° {TEST_NN_MODEL_NAME} ç±»")
            print("âœ“ è½¬æ¢åçš„CUDAä»£ç åŠ è½½å®Œæˆ")
        
        # 5. ä¸‰æ–¹å¯¹æ¯”æµ‹è¯•ï¼šå‚è€ƒå®ç° vs Goldenå®ç° vs è½¬æ¢åçš„å®ç°
        print("\nå¼€å§‹ä¸‰æ–¹å¯¹æ¯”æµ‹è¯•...")
        all_passed = True
        
        for i, input_tensor in enumerate(input_tensors):
            # å‚è€ƒè¾“å‡ºï¼ˆTritonï¼‰
            ref_output = RefModel()(*input_tensor)
            
            # Goldenè¾“å‡ºï¼ˆæ‰‹å†™CUDAï¼‰
            golden_output = GoldenModel()(*input_tensor)
            
            # è½¬æ¢è¾“å‡ºï¼ˆè½¬æ¢åçš„CUDAï¼‰
            converted_output = ConvertedModel()(*input_tensor)
            
            # æ£€æŸ¥å‚è€ƒ vs Golden
            ref_golden_match = torch.allclose(golden_output, ref_output, atol=1e-3, rtol=1e-3)
            
            # æ£€æŸ¥è½¬æ¢ vs å‚è€ƒ
            converted_ref_match = torch.allclose(converted_output, ref_output, atol=1e-3, rtol=1e-3)
            
            # æ£€æŸ¥è½¬æ¢ vs Golden
            converted_golden_match = torch.allclose(converted_output, golden_output, atol=1e-3, rtol=1e-3)
            
            print(f"  æµ‹è¯• {i+1}:")
            print(f"    å‚è€ƒ vs Golden: {'âœ“' if ref_golden_match else 'âœ—'}")
            print(f"    è½¬æ¢ vs å‚è€ƒ:   {'âœ“' if converted_ref_match else 'âœ—'}")
            print(f"    è½¬æ¢ vs Golden: {'âœ“' if converted_golden_match else 'âœ—'}")
            
            if not (ref_golden_match and converted_ref_match and converted_golden_match):
                all_passed = False
                if not ref_golden_match:
                    max_diff = torch.max(torch.abs(golden_output - ref_output)).item()
                    print(f"      å‚è€ƒvs Goldenæœ€å¤§å·®å¼‚: {max_diff}")
                if not converted_ref_match:
                    max_diff = torch.max(torch.abs(converted_output - ref_output)).item()
                    print(f"      è½¬æ¢vså‚è€ƒæœ€å¤§å·®å¼‚: {max_diff}")
                if not converted_golden_match:
                    max_diff = torch.max(torch.abs(converted_output - golden_output)).item()
                    print(f"      è½¬æ¢vs Goldenæœ€å¤§å·®å¼‚: {max_diff}")
        
        if all_passed:
            print("\nğŸ‰ Goldenæ ‡å‡†è¯„æµ‹é€šè¿‡ï¼æ‰€æœ‰å®ç°ç»“æœä¸€è‡´")
        else:
            print("\nâŒ Goldenæ ‡å‡†è¯„æµ‹å¤±è´¥ï¼å­˜åœ¨å®ç°å·®å¼‚")
        
        return all_passed
        
    except Exception as e:
        print(f"âŒ Goldenè¯„æµ‹å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def show_usage():
    """æ˜¾ç¤ºä½¿ç”¨è¯´æ˜"""
    print("Tritonåˆ°CUDAè½¬æ¢è¯„æµ‹ç³»ç»Ÿä½¿ç”¨è¯´æ˜:")
    print("")
    print("åŸºæœ¬ç”¨æ³•:")
    print("  python eval.py --file vecadd.py        # è¯„æµ‹å•ä¸ªæ–‡ä»¶")
    print("  python eval.py --all                   # è¯„æµ‹æ‰€æœ‰æ–‡ä»¶")
    print("  python eval.py --all --verbose         # è¯¦ç»†æ¨¡å¼è¯„æµ‹æ‰€æœ‰æ–‡ä»¶")
    print("")
    print("ç‰¹æ®ŠåŠŸèƒ½:")
    print("  python eval.py --network              # æµ‹è¯•APIç«¯ç‚¹å¯ç”¨æ€§")
    print("  python eval.py --golden               # Goldenæ ‡å‡†è¯„æµ‹")
    print("  python eval.py --simple               # ç®€å•vecaddè¯„æµ‹")
    print("")
    print("APIè¿é€šæ€§æµ‹è¯•:")
    print("  å½“é‡åˆ°è½¬æ¢å¤±è´¥æ—¶ï¼Œå¯ä»¥å…ˆè¿è¡Œ --network æµ‹è¯•APIæ˜¯å¦å¯ç”¨")
    print("  ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹APIç›¸å…³é”™è¯¯å¹¶æä¾›è§£å†³å»ºè®®")
    print("")

def cleanup():
    """æ¢å¤åŸå§‹å·¥ä½œç›®å½•"""
    os.chdir(original_cwd)

if __name__ == "__main__":
    try:
        parser = argparse.ArgumentParser(description="Tritonåˆ°CUDAè½¬æ¢è¯„æµ‹ç³»ç»Ÿ")
        parser.add_argument("--file", type=str, help="è¯„æµ‹å•ä¸ªæ–‡ä»¶")
        parser.add_argument("--all", action="store_true", help="è¯„æµ‹æ‰€æœ‰æ–‡ä»¶")
        parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
        parser.add_argument("--network", action="store_true", help="æµ‹è¯•APIç«¯ç‚¹å¯ç”¨æ€§")
        parser.add_argument("--golden", action="store_true", help="Goldenæ ‡å‡†è¯„æµ‹")
        parser.add_argument("--simple", action="store_true", help="ç®€å•vecaddè¯„æµ‹")
        parser.add_argument("--help-usage", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†ä½¿ç”¨è¯´æ˜")
        
        args = parser.parse_args()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å®‰è£…requestsåº“
        try:
            import requests
        except ImportError:
            print("âŒ éœ€è¦å®‰è£…requestsåº“")
            print("è¯·è¿è¡Œ: pip install requests")
            exit(1)
        
        if args.help_usage:
            # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
            show_usage()
            exit(0)
        
        elif args.network:
            # APIè¿é€šæ€§æµ‹è¯•
            success = test_api_connectivity()
            if not success:
                print("\nå»ºè®®:")
                print("1. éªŒè¯APIå¯†é’¥é…ç½®")
                print("2. ç¡®è®¤é˜²ç«å¢™è®¾ç½®")
                print("3. å°è¯•ä½¿ç”¨VPNæˆ–ä»£ç†")
                print("4. æ£€æŸ¥APIæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
            exit(0)
        
        elif args.file:
            # å•æ–‡ä»¶è¯„æµ‹
            success, error_messages = eval_single_file(args.file, args.verbose)
            if success:
                print(f"\nâœ… {args.file} è¯„æµ‹é€šè¿‡")
            else:
                print(f"\nâŒ {args.file} è¯„æµ‹å¤±è´¥")
                if error_messages:
                    # æ£€æŸ¥æ˜¯å¦æœ‰APIç›¸å…³é”™è¯¯
                    api_errors = []
                    for error in error_messages:
                        error_type = classify_api_error(error)
                        if error_type in ["TimeoutError", "ConnectionError", "AuthenticationError", "NetworkError"]:
                            api_errors.append(error_type)
                    
                    if api_errors:
                        print("\nğŸ’¡ APIç›¸å…³é”™è¯¯è§£å†³å»ºè®®:")
                        print("1. è¿è¡Œ --network å‚æ•°æµ‹è¯•APIç«¯ç‚¹å¯ç”¨æ€§")
                        print("2. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®")
                        print("3. ç¡®è®¤APIæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
                        print("4. å°è¯•åˆ‡æ¢ç½‘ç»œç¯å¢ƒæˆ–ä½¿ç”¨ä»£ç†")
        
        elif args.all:
            # æ‰¹é‡è¯„æµ‹
            success = eval_all_files(args.verbose)
            if success:
                print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶è¯„æµ‹é€šè¿‡!")
            else:
                print("\nâŒ éƒ¨åˆ†æ–‡ä»¶è¯„æµ‹å¤±è´¥")
        
        elif args.golden:
            # Goldenæ ‡å‡†è¯„æµ‹
            success = eval_golden()
            if success:
                print("\nğŸ‰ Goldenæ ‡å‡†è¯„æµ‹é€šè¿‡!")
            else:
                print("\nâŒ Goldenæ ‡å‡†è¯„æµ‹å¤±è´¥")
        
        elif args.simple:
            # ç®€å•vecaddè¯„æµ‹
            success = eval_simple()
            if success:
                print("\nâœ… ç®€å•vecaddè¯„æµ‹é€šè¿‡")
            else:
                print("\nâŒ ç®€å•vecaddè¯„æµ‹å¤±è´¥")
        
        else:
            # é»˜è®¤è¯„æµ‹å•ä¸ªæ–‡ä»¶
            success, error_messages = eval_single_file("mat_transpose.py", verbose=True)
            if success:
                print("\nâœ… é»˜è®¤è¯„æµ‹é€šè¿‡")
            else:
                print("\nâŒ é»˜è®¤è¯„æµ‹å¤±è´¥")
                if error_messages:
                    # æ£€æŸ¥æ˜¯å¦æœ‰APIç›¸å…³é”™è¯¯
                    api_errors = []
                    for error in error_messages:
                        error_type = classify_api_error(error)
                        if error_type in ["TimeoutError", "ConnectionError", "AuthenticationError", "NetworkError"]:
                            api_errors.append(error_type)
                    
                    if api_errors:
                        print("\nğŸ’¡ APIç›¸å…³é”™è¯¯è§£å†³å»ºè®®:")
                        print("1. è¿è¡Œ --network å‚æ•°æµ‹è¯•APIç«¯ç‚¹å¯ç”¨æ€§")
                        print("2. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®")
                        print("3. ç¡®è®¤APIæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
                        print("4. å°è¯•åˆ‡æ¢ç½‘ç»œç¯å¢ƒæˆ–ä½¿ç”¨ä»£ç†")
        
    except Exception as e:
        print(f"è¯„æµ‹è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ç¡®ä¿æ¢å¤åŸå§‹å·¥ä½œç›®å½•
        cleanup()
    