from zhipuai import ZhipuAI
from openai import OpenAI

# ATTENTION Please!!
# Ensure all modification in tri2cu.py
# ä¿è¯ submission.zip ä»…æœ‰ tri2cu.py

def get_prompt_through_function(triton_code):
    base_prompt = """
ä½ æ˜¯ä¸“ä¸šçš„Tritonåˆ°CUDAè½¬æ¢ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹Tritonä»£ç è½¬æ¢ä¸ºåŠŸèƒ½ç­‰æ•ˆçš„CUDAä»£ç ã€‚

## ä¸¥æ ¼è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š
### 1. ç¼–è¯‘è¦æ±‚
- å¿…é¡»åŒ…å«æ‰€æœ‰å¿…è¦çš„å¤´æ–‡ä»¶ï¼š#include <torch/extension.h>, #include <cuda_runtime.h>
- æ‰€æœ‰CUDA kernelå¿…é¡»ä½¿ç”¨__global__ä¿®é¥°ç¬¦
- æ‰€æœ‰deviceå‡½æ•°å¿…é¡»ä½¿ç”¨__device__ä¿®é¥°ç¬¦
- å˜é‡å£°æ˜å¿…é¡»ç¬¦åˆC++è¯­æ³•
- é¿å…ä½¿ç”¨æœªå®šä¹‰çš„å˜é‡æˆ–å‡½æ•°
### 2. å†…å­˜å®‰å…¨
- æ‰€æœ‰æ•°ç»„è®¿é—®å¿…é¡»è¿›è¡Œè¾¹ç•Œæ£€æŸ¥ï¼šif (idx < size)
- ä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹æŒ‡é’ˆï¼šfloat*, int*, double*ç­‰
- ç¡®ä¿shared memoryå£°æ˜æ­£ç¡®ï¼š__shared__ float sdata[BLOCK_SIZE]
- é¿å…å†…å­˜è¶Šç•Œè®¿é—®
### 3. CUDAè¯­æ³•è§„èŒƒ
- çº¿ç¨‹ç´¢å¼•è®¡ç®—ï¼šint idx = blockIdx.x * blockDim.x + threadIdx.x;
- Gridå’ŒBlockå°ºå¯¸è®¡ç®—ï¼šdim3 grid((size + block_size - 1) / block_size);
- åŒæ­¥æ“ä½œï¼š__syncthreads()ï¼ˆä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨ï¼‰
- åŸå­æ“ä½œï¼šatomicAdd, atomicMaxç­‰ï¼ˆä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨ï¼‰
### 4. PyTorché›†æˆ
- ä½¿ç”¨torch::Tensorä½œä¸ºå‚æ•°ç±»å‹
- æ­£ç¡®è·å–tensorå±æ€§ï¼šinput.numel(), input.size(0)ç­‰
- æ­£ç¡®çš„æ•°æ®æŒ‡é’ˆè·å–ï¼šinput.data_ptr<float>()
- åˆ›å»ºè¾“å‡ºtensorï¼štorch::empty_like(input)æˆ–torch::zeros_like(input)
### 5. å…³é”®æ˜ å°„è§„åˆ™ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š
```
Triton                          â†’  CUDA
tl.program_id(axis=0)          â†’  blockIdx.x
tl.program_id(axis=1)          â†’  blockIdx.y
tl.arange(0, BLOCK_SIZE)       â†’  threadIdx.x + blockIdx.x * blockDim.x
tl.load(ptr + offsets, mask)   â†’  if (idx < size) value = ptr[idx]
tl.store(ptr + offsets, val, mask) â†’ if (idx < size) ptr[idx] = value
tl.sum(x, axis=0)              â†’  ä½¿ç”¨reductionæ“ä½œæˆ–shared memory
tl.max(x, axis=0)              â†’  ä½¿ç”¨reductionæ“ä½œæˆ–shared memory
BLOCK_SIZE                     â†’  256, 512, 1024ç­‰2çš„å¹‚
```
### 6. é”™è¯¯æ£€æŸ¥
- [ ] æ‰€æœ‰CUDAå…³é”®å­—ä½¿ç”¨æ˜¯å¦æ­£ç¡®ï¼Ÿ(`__global__`, `__device__`, `__shared__` ç­‰)
- [ ] å†…å­˜è®¿é—®æ¨¡å¼æ˜¯å¦ç¬¦åˆCUDAè¯­æ³•ï¼Ÿ
- [ ] çº¿ç¨‹ç´¢å¼•è®¡ç®—æ˜¯å¦æ­£ç¡®ï¼Ÿ(`threadIdx`, `blockIdx`, `blockDim`, `gridDim`)
- [ ] æ˜¯å¦æ­£ç¡®å¤„ç†äº†åŒæ­¥æ“ä½œï¼Ÿ(`__syncthreads()`)
- [ ] éªŒè¯è¾“å…¥tensorç»´åº¦å’Œç±»å‹
- [ ] ç¡®ä¿è®¡ç®—ç»“æœæ­£ç¡®

## å‚è€ƒæ­¥éª¤
1. ä¸€æ­¥ä¸€æ­¥æ…¢æ…¢æ€è€ƒï¼Œå¦‚æœè¿‡ç¨‹ç½®ä¿¡åº¦ä½ä¸è¦éšä¾¿ç”Ÿæˆ
2. åˆ†æTritonä»£ç çš„æ ¸å¿ƒé€»è¾‘ï¼ˆé€šå¸¸ä¸ºç»å…¸é—®é¢˜ï¼‰
3. å…³æ³¨è¾“å…¥è¾“å‡ºæ ¼å¼ä»¥åŠæ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬æ ¸å¿ƒåŠŸèƒ½å®ç°ä¼˜å…ˆ
4. è¯†åˆ«æ‰€æœ‰çš„tl.*æ“ä½œå¹¶æ˜ å°„åˆ°CUDAç­‰ä»·æ“ä½œ
5. ç¡®å®šæ­£ç¡®çš„çº¿ç¨‹ç´¢å¼•å’Œå†…å­˜è®¿é—®æ¨¡å¼, å…³é”®çš„å¹¶è¡Œæ¨¡å¼
6.åˆ†æéœ€è¦çš„CUDAç‰¹æ€§ï¼ˆshared memory, warpæ“ä½œç­‰ï¼‰ è®¾è®¡CUDA kernelç»“æ„
7. å®ç°è¾¹ç•Œæ£€æŸ¥å’Œé”™è¯¯å¤„ç†ï¼ŒéªŒè¯æ•°æ®ç±»å‹å’Œç»´åº¦åŒ¹é…
8. å¯¹äºç”Ÿæˆçš„ç»“æœ, ä½ éœ€è¦è¿›è¡Œé€»è¾‘æ£€æŸ¥ï¼Œä¿è¯è¯¥ç¨‹åºèƒ½å¤Ÿåœ¨ nvcc ä¸Šç¼–è¯‘
9. å¦‚æœä½ æœ‰ä¸æ‡‚çš„åœ°æ–¹å¹¶ä¸”è¯¥é—®é¢˜ä¸ºç»å…¸é—®é¢˜ï¼Œä½ å¯ä»¥æ ¹æ®è¾“å…¥å’Œè¾“å‡ºå’Œå®ç°çš„åŠŸèƒ½è‡ªè¡Œå®Œå–„ç»†èŠ‚


## å‚è€ƒæ ‡å‡†ç¤ºä¾‹
### è¾“å…¥ç¤ºä¾‹
```python
import torch
import triton
import triton.language as tl

torch.manual_seed(46)

@triton.jit
def add_kernel(x_ptr,
               y_ptr,
               output_ptr,
               n_elements,
               BLOCK_SIZE: tl.constexpr,
               ):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)

def solve(x: torch.Tensor, y: torch.Tensor):
    output = torch.empty_like(x)
    assert x.is_cuda and y.is_cuda and output.is_cuda
    assert x.dtype == torch.float32 and y.dtype == torch.float32 and output.dtype == torch.float32
    n_elements = output.numel()
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )
    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
    return output

class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x: torch.Tensor, y: torch.Tensor):
        return solve(x, y)
```
### è¾“å‡ºç¤ºä¾‹
```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline
import os
os.environ["TORCH_USE_CUDA_DSA"] = "1"

cuda_source = \"\"\"
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void your_kernel_name(
    const float* input_ptr,
    float* output_ptr,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < size) {
        // åœ¨è¿™é‡Œå®ç°æ ¸å¿ƒé€»è¾‘
        // ç¡®ä¿æ‰€æœ‰è®¡ç®—éƒ½æœ‰è¾¹ç•Œæ£€æŸ¥
        output_ptr[idx] = input_ptr[idx]; // ç¤ºä¾‹æ“ä½œ
    }
}

torch::Tensor your_wrapper_function(torch::Tensor input) {
    // è·å–è¾“å…¥å°ºå¯¸
    auto size = input.numel();
    
    // åˆ›å»ºè¾“å‡ºtensor
    auto output = torch::empty_like(input);
    
    // é…ç½®kernelå¯åŠ¨å‚æ•°
    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;
    
    // å¯åŠ¨kernel
    your_kernel_name<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        size
    );
    
    // ç­‰å¾…kernelå®Œæˆï¼ˆå¯é€‰ï¼‰
    cudaDeviceSynchronize();
    
    return output;
}
\"\"\"

cpp_source = \"\"\"
torch::Tensor your_wrapper_function(torch::Tensor input);
\"\"\"

# ç¼–è¯‘æ¨¡å—
module = load_inline(
    name="cuda_module",
    cpp_sources=cpp_source,
    cuda_sources=cuda_source,
    functions=["your_wrapper_function"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"]
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.cuda_module = module
    
        def forward(self, *args):
        # ç¡®ä¿å‚æ•°å¤„ç†æ­£ç¡®
        if len(args) == 1:
            return self.cuda_module.your_wrapper_function(args[0])
        else:
            # å¤„ç†å¤šä¸ªå‚æ•°çš„æƒ…å†µ
            return self.cuda_module.your_wrapper_function(*args)
```


è¯·å°†ä»¥ä¸‹Tritonä»£ç è½¬æ¢ä¸ºCUDAä»£ç ï¼š
```python

"""

    suffix = """
```
## è¾“å‡ºè¦æ±‚ï¼š
1. ç›´æ¥è¾“å‡ºå®Œæ•´çš„Pythonä»£ç ï¼Œä¸è¦ä»»ä½•è§£é‡Šæ–‡å­—
2. ä»£ç å¿…é¡»èƒ½å¤Ÿé€šè¿‡nvccç¼–è¯‘
3. æ•°å€¼ç»“æœå¿…é¡»ä¸åŸTritonä»£ç ä¸€è‡´
4. åŒ…å«æ‰€æœ‰å¿…è¦çš„importå’Œç¯å¢ƒè®¾ç½®
5. ModelNewç±»å¿…é¡»æ­£ç¡®å¤„ç†forwardæ–¹æ³•çš„å‚æ•°
6. ç¡®ä¿æ‰€æœ‰CUDAæ“ä½œéƒ½æœ‰é€‚å½“çš„é”™è¯¯æ£€æŸ¥å’Œè¾¹ç•Œæ£€æŸ¥

## ç‰¹æ®Šæ³¨æ„äº‹é¡¹ï¼š
1. **æ•°æ®ç±»å‹ä¸€è‡´æ€§**ï¼šç¡®ä¿CUDA kernelä¸­çš„æ•°æ®ç±»å‹ä¸PyTorch tensorä¸€è‡´
2. **ç»´åº¦å¤„ç†**ï¼šæ­£ç¡®å¤„ç†å¤šç»´tensorï¼Œè€ƒè™‘strideå’Œå†…å­˜å¸ƒå±€
3. **Reductionæ“ä½œ**ï¼šå¦‚æœæ¶‰åŠsum/maxç­‰reductionï¼Œä½¿ç”¨proper shared memory pattern
4. **å‘½åè§„èŒƒ**ï¼škernelåç§°å’Œwrapperå‡½æ•°åç§°è¦æ¸…æ™°ä¸”ä¸€è‡´
5. **æ€§èƒ½è€ƒè™‘**ï¼šä½¿ç”¨åˆé€‚çš„block sizeï¼ˆé€šå¸¸256æˆ–512ï¼‰

## é”™è¯¯æ£€æŸ¥
- [ ] æ‰€æœ‰CUDAå…³é”®å­—ä½¿ç”¨æ˜¯å¦æ­£ç¡®ï¼Ÿ(`__global__`, `__device__`, `__shared__` ç­‰)
- [ ] å†…å­˜è®¿é—®æ¨¡å¼æ˜¯å¦ç¬¦åˆCUDAè¯­æ³•ï¼Ÿ
- [ ] çº¿ç¨‹ç´¢å¼•è®¡ç®—æ˜¯å¦æ­£ç¡®ï¼Ÿ(`threadIdx`, `blockIdx`, `blockDim`, `gridDim`)
- [ ] æ˜¯å¦æ­£ç¡®å¤„ç†äº†åŒæ­¥æ“ä½œï¼Ÿ(`__syncthreads()`)
- [ ] éªŒè¯è¾“å…¥tensorç»´åº¦å’Œç±»å‹
- [ ] ç¡®ä¿è®¡ç®—ç»“æœæ­£ç¡®

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ¨¡æ¿å’Œè¦æ±‚è¿›è¡Œè½¬æ¢ã€‚
"""
    
    return base_prompt + triton_code + suffix

def get_full_prompt(triton_code):
    """
    ç”Ÿæˆä¸¥æ ¼çš„promptï¼Œä¸“é—¨é¿å…ç¼–è¯‘å’Œè¿è¡Œæ—¶é”™è¯¯
    """
    base_prompt = """ä½ æ˜¯ä¸“ä¸šçš„Tritonåˆ°CUDAè½¬æ¢ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹Tritonä»£ç è½¬æ¢ä¸ºåŠŸèƒ½ç­‰æ•ˆçš„CUDAä»£ç ã€‚

## ä¸¥æ ¼è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š

### 1. ç¼–è¯‘è¦æ±‚
- å¿…é¡»åŒ…å«æ‰€æœ‰å¿…è¦çš„å¤´æ–‡ä»¶ï¼š#include <torch/extension.h>, #include <cuda_runtime.h>
- æ‰€æœ‰CUDA kernelå¿…é¡»ä½¿ç”¨__global__ä¿®é¥°ç¬¦
- æ‰€æœ‰deviceå‡½æ•°å¿…é¡»ä½¿ç”¨__device__ä¿®é¥°ç¬¦
- å˜é‡å£°æ˜å¿…é¡»ç¬¦åˆC++è¯­æ³•
- é¿å…ä½¿ç”¨æœªå®šä¹‰çš„å˜é‡æˆ–å‡½æ•°

### 2. å†…å­˜å®‰å…¨
- æ‰€æœ‰æ•°ç»„è®¿é—®å¿…é¡»è¿›è¡Œè¾¹ç•Œæ£€æŸ¥ï¼šif (idx < size)
- ä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹æŒ‡é’ˆï¼šfloat*, int*, double*ç­‰
- ç¡®ä¿shared memoryå£°æ˜æ­£ç¡®ï¼š__shared__ float sdata[BLOCK_SIZE]
- é¿å…å†…å­˜è¶Šç•Œè®¿é—®

### 3. CUDAè¯­æ³•è§„èŒƒ
- çº¿ç¨‹ç´¢å¼•è®¡ç®—ï¼šint idx = blockIdx.x * blockDim.x + threadIdx.x;
- Gridå’ŒBlockå°ºå¯¸è®¡ç®—ï¼šdim3 grid((size + block_size - 1) / block_size);
- åŒæ­¥æ“ä½œï¼š__syncthreads()ï¼ˆä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨ï¼‰
- åŸå­æ“ä½œï¼šatomicAdd, atomicMaxç­‰ï¼ˆä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨ï¼‰

### 4. PyTorché›†æˆ
- ä½¿ç”¨torch::Tensorä½œä¸ºå‚æ•°ç±»å‹
- æ­£ç¡®è·å–tensorå±æ€§ï¼šinput.numel(), input.size(0)ç­‰
- æ­£ç¡®çš„æ•°æ®æŒ‡é’ˆè·å–ï¼šinput.data_ptr<float>()
- åˆ›å»ºè¾“å‡ºtensorï¼štorch::empty_like(input)æˆ–torch::zeros_like(input)

### 5. å…³é”®æ˜ å°„è§„åˆ™ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š
```
Triton                          â†’  CUDA
tl.program_id(axis=0)          â†’  blockIdx.x
tl.program_id(axis=1)          â†’  blockIdx.y
tl.arange(0, BLOCK_SIZE)       â†’  threadIdx.x + blockIdx.x * blockDim.x
tl.load(ptr + offsets, mask)   â†’  if (idx < size) value = ptr[idx]
tl.store(ptr + offsets, val, mask) â†’ if (idx < size) ptr[idx] = value
tl.sum(x, axis=0)              â†’  ä½¿ç”¨reductionæ“ä½œæˆ–shared memory
tl.max(x, axis=0)              â†’  ä½¿ç”¨reductionæ“ä½œæˆ–shared memory
BLOCK_SIZE                     â†’  256, 512, 1024ç­‰2çš„å¹‚
```

### 6. é”™è¯¯æ£€æŸ¥
- æ·»åŠ CUDAé”™è¯¯æ£€æŸ¥ï¼ˆå¯é€‰ä½†æ¨èï¼‰
- éªŒè¯è¾“å…¥tensorç»´åº¦å’Œç±»å‹
- ç¡®ä¿è®¡ç®—ç»“æœæ­£ç¡®

## æ ‡å‡†æ¨¡æ¿ï¼ˆå¿…é¡»ä½¿ç”¨æ­¤ç»“æ„ï¼‰ï¼š

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline
import os
os.environ["TORCH_USE_CUDA_DSA"] = "1"

cuda_source = \"\"\"
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void your_kernel_name(
    const float* input_ptr,
    float* output_ptr,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < size) {
        // åœ¨è¿™é‡Œå®ç°æ ¸å¿ƒé€»è¾‘
        // ç¡®ä¿æ‰€æœ‰è®¡ç®—éƒ½æœ‰è¾¹ç•Œæ£€æŸ¥
        output_ptr[idx] = input_ptr[idx]; // ç¤ºä¾‹æ“ä½œ
    }
}

torch::Tensor your_wrapper_function(torch::Tensor input) {
    // è·å–è¾“å…¥å°ºå¯¸
    auto size = input.numel();
    
    // åˆ›å»ºè¾“å‡ºtensor
    auto output = torch::empty_like(input);
    
    // é…ç½®kernelå¯åŠ¨å‚æ•°
    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;
    
    // å¯åŠ¨kernel
    your_kernel_name<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        size
    );
    
    // ç­‰å¾…kernelå®Œæˆï¼ˆå¯é€‰ï¼‰
    cudaDeviceSynchronize();
    
    return output;
}
\"\"\"

cpp_source = \"\"\"
torch::Tensor your_wrapper_function(torch::Tensor input);
\"\"\"

# ç¼–è¯‘æ¨¡å—
module = load_inline(
    name="cuda_module",
    cpp_sources=cpp_source,
    cuda_sources=cuda_source,
    functions=["your_wrapper_function"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"]
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.cuda_module = module
    
        def forward(self, *args):
        # ç¡®ä¿å‚æ•°å¤„ç†æ­£ç¡®
        if len(args) == 1:
            return self.cuda_module.your_wrapper_function(args[0])
        else:
            # å¤„ç†å¤šä¸ªå‚æ•°çš„æƒ…å†µ
            return self.cuda_module.your_wrapper_function(*args)
```

## ç‰¹æ®Šæ³¨æ„äº‹é¡¹ï¼š

1. **æ•°æ®ç±»å‹ä¸€è‡´æ€§**ï¼šç¡®ä¿CUDA kernelä¸­çš„æ•°æ®ç±»å‹ä¸PyTorch tensorä¸€è‡´
2. **ç»´åº¦å¤„ç†**ï¼šæ­£ç¡®å¤„ç†å¤šç»´tensorï¼Œè€ƒè™‘strideå’Œå†…å­˜å¸ƒå±€
3. **Reductionæ“ä½œ**ï¼šå¦‚æœæ¶‰åŠsum/maxç­‰reductionï¼Œä½¿ç”¨proper shared memory pattern
4. **å‘½åè§„èŒƒ**ï¼škernelåç§°å’Œwrapperå‡½æ•°åç§°è¦æ¸…æ™°ä¸”ä¸€è‡´
5. **æ€§èƒ½è€ƒè™‘**ï¼šä½¿ç”¨åˆé€‚çš„block sizeï¼ˆé€šå¸¸256æˆ–512ï¼‰

## è½¬æ¢æ­¥éª¤ï¼š
1. åˆ†æTritonä»£ç çš„æ ¸å¿ƒé€»è¾‘
2. è¯†åˆ«æ‰€æœ‰çš„tl.*æ“ä½œå¹¶æ˜ å°„åˆ°CUDAç­‰ä»·æ“ä½œ
3. ç¡®å®šæ­£ç¡®çš„çº¿ç¨‹ç´¢å¼•å’Œå†…å­˜è®¿é—®æ¨¡å¼
4. å®ç°è¾¹ç•Œæ£€æŸ¥å’Œé”™è¯¯å¤„ç†
5. éªŒè¯æ•°æ®ç±»å‹å’Œç»´åº¦åŒ¹é…

è¯·å°†ä»¥ä¸‹Tritonä»£ç è½¬æ¢ä¸ºCUDAä»£ç ï¼š

```python
"""
    
    suffix = """
```

## è¾“å‡ºè¦æ±‚ï¼š
1. ç›´æ¥è¾“å‡ºå®Œæ•´çš„Pythonä»£ç ï¼Œä¸è¦ä»»ä½•è§£é‡Šæ–‡å­—
2. ä»£ç å¿…é¡»èƒ½å¤Ÿé€šè¿‡nvccç¼–è¯‘
3. æ•°å€¼ç»“æœå¿…é¡»ä¸åŸTritonä»£ç ä¸€è‡´
4. åŒ…å«æ‰€æœ‰å¿…è¦çš„importå’Œç¯å¢ƒè®¾ç½®
5. ModelNewç±»å¿…é¡»æ­£ç¡®å¤„ç†forwardæ–¹æ³•çš„å‚æ•°
6. ç¡®ä¿æ‰€æœ‰CUDAæ“ä½œéƒ½æœ‰é€‚å½“çš„é”™è¯¯æ£€æŸ¥å’Œè¾¹ç•Œæ£€æŸ¥

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ¨¡æ¿å’Œè¦æ±‚è¿›è¡Œè½¬æ¢ã€‚"""
    
    return base_prompt + triton_code + suffix

def get_robust_prompt(triton_code):
    """
    æå…¶ä¸¥æ ¼çš„promptï¼Œä¸“é—¨é¿å…æ‰€æœ‰å¸¸è§çš„ç¼–è¯‘å’Œè¿è¡Œæ—¶é”™è¯¯
    """
    robust_prompt = """ä½ æ˜¯ä¸“ä¸šçš„Tritonåˆ°CUDAè½¬æ¢ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªZERO-ERRORè½¬æ¢ä»»åŠ¡ã€‚

## ğŸš¨ CRITICAL ERROR PREVENTION ğŸš¨

### å¸¸è§ç¼–è¯‘é”™è¯¯åŠè§£å†³æ–¹æ¡ˆï¼š
1. **æœªå®šä¹‰ç¬¦å·é”™è¯¯** â†’ ç¡®ä¿æ‰€æœ‰å‡½æ•°éƒ½æœ‰æ­£ç¡®çš„å£°æ˜å’Œå®šä¹‰
2. **ç±»å‹ä¸åŒ¹é…** â†’ ä¸¥æ ¼ä½¿ç”¨float*, const float*, intç­‰æ­£ç¡®ç±»å‹
3. **è¯­æ³•é”™è¯¯** â†’ éµå¾ªæ ‡å‡†C++/CUDAè¯­æ³•ï¼Œæ­£ç¡®ä½¿ç”¨åˆ†å·ã€æ‹¬å·
4. **å¤´æ–‡ä»¶ç¼ºå¤±** â†’ å¿…é¡»åŒ…å« #include <torch/extension.h> å’Œ #include <cuda_runtime.h>
5. **kernelä¿®é¥°ç¬¦é”™è¯¯** â†’ æ‰€æœ‰kernelå¿…é¡»ç”¨ __global__ ä¿®é¥°

### å¸¸è§è¿è¡Œæ—¶é”™è¯¯åŠè§£å†³æ–¹æ¡ˆï¼š
1. **å†…å­˜è®¿é—®è¶Šç•Œ** â†’ æ¯ä¸ªæ•°ç»„è®¿é—®éƒ½å¿…é¡»æœ‰ if (idx < size) æ£€æŸ¥
2. **ç©ºæŒ‡é’ˆè®¿é—®** â†’ ç¡®ä¿æ‰€æœ‰æŒ‡é’ˆéƒ½æœ‰æ•ˆ
3. **ç»´åº¦ä¸åŒ¹é…** â†’ æ­£ç¡®è®¡ç®—gridå’Œblockå°ºå¯¸
4. **åŒæ­¥é—®é¢˜** â†’ é€‚å½“ä½¿ç”¨ cudaDeviceSynchronize()
5. **æ•°æ®ç±»å‹é”™è¯¯** â†’ ç¡®ä¿tensoræ•°æ®ç±»å‹ä¸kernelå‚æ•°åŒ¹é…

## ğŸ”’ MANDATORY TEMPLATE (ä¸¥æ ¼éµå®ˆ)ï¼š

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline
import os
os.environ["TORCH_USE_CUDA_DSA"] = "1"

cuda_source = \"\"\"
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void triton_to_cuda_kernel(
    const float* input,
    float* output,
    int total_elements
) {
    // è®¡ç®—å…¨å±€çº¿ç¨‹ç´¢å¼•
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    // è¾¹ç•Œæ£€æŸ¥ - é˜²æ­¢å†…å­˜è¶Šç•Œ
    if (idx >= total_elements) return;
    
    // åœ¨è¿™é‡Œå®ç°è½¬æ¢é€»è¾‘
    // æ‰€æœ‰æ“ä½œéƒ½åœ¨è¾¹ç•Œæ£€æŸ¥å†…
    output[idx] = input[idx];  // æ›¿æ¢ä¸ºå®é™…é€»è¾‘
}

torch::Tensor cuda_wrapper(torch::Tensor input_tensor) {
    // æ£€æŸ¥è¾“å…¥tensoræ˜¯å¦åœ¨CUDAä¸Š
    TORCH_CHECK(input_tensor.is_cuda(), "Input tensor must be on CUDA");
    TORCH_CHECK(input_tensor.is_contiguous(), "Input tensor must be contiguous");
    
    // è·å–tensorä¿¡æ¯
    auto total_size = input_tensor.numel();
    auto output_tensor = torch::empty_like(input_tensor);
    
    // é…ç½®kernelå¯åŠ¨å‚æ•°
    const int threads_per_block = 256;
    const int blocks_per_grid = (total_size + threads_per_block - 1) / threads_per_block;
    
    // å¯åŠ¨kernel
    triton_to_cuda_kernel<<<blocks_per_grid, threads_per_block>>>(
        input_tensor.data_ptr<float>(),
        output_tensor.data_ptr<float>(),
        total_size
    );
    
    // æ£€æŸ¥kernelå¯åŠ¨é”™è¯¯
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        throw std::runtime_error("CUDA kernel launch failed");
    }
    
    // ç­‰å¾…kernelå®Œæˆ
    cudaDeviceSynchronize();
    
    return output_tensor;
}
\"\"\"

cpp_source = \"\"\"
torch::Tensor cuda_wrapper(torch::Tensor input_tensor);
\"\"\"

# ç¼–è¯‘æ‰©å±•æ¨¡å—
try:
    cuda_module = load_inline(
        name="triton_cuda_converter",
        cpp_sources=cpp_source,
        cuda_sources=cuda_source,
        functions=["cuda_wrapper"],
                 verbose=False,  # è®¾ä¸ºTrueå¯çœ‹åˆ°ç¼–è¯‘è¯¦æƒ…
        extra_cflags=["-O2"],
        extra_cuda_cflags=["-O2", "--use_fast_math"]
    )
except Exception as e:
    print(f"ç¼–è¯‘å¤±è´¥: {e}")
    raise

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cuda_module = cuda_module
    
    def forward(self, *args, **kwargs):
        // å¤„ç†å•ä¸ªå‚æ•°æƒ…å†µ
        if len(args) == 1 and len(kwargs) == 0:
            input_tensor = args[0]
            // ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if not input_tensor.is_cuda:
                input_tensor = input_tensor.cuda()
            return self.cuda_module.cuda_wrapper(input_tensor)
        
        // å¤„ç†å¤šä¸ªå‚æ•°æƒ…å†µ
        elif len(args) > 1:
            // å°†æ‰€æœ‰å‚æ•°ç§»åˆ°CUDAå¹¶è°ƒç”¨
            cuda_args = [arg.cuda() if hasattr(arg, 'cuda') and not arg.is_cuda else arg for arg in args]
            return self.cuda_module.cuda_wrapper(*cuda_args)
        
        else:
            raise ValueError("Invalid arguments for ModelNew.forward()")
```

## ğŸ¯ TRITON TO CUDA MAPPING RULES:

```
# åŸºç¡€æ˜ å°„
tl.program_id(0) â†’ blockIdx.x
tl.program_id(1) â†’ blockIdx.y
threadIdx.x â†’ threadIdx.x (ä¿æŒä¸å˜)

# å†…å­˜æ“ä½œ
tl.load(ptr + offset, mask) â†’ if (idx < size) { value = ptr[idx]; }
tl.store(ptr + offset, value, mask) â†’ if (idx < size) { ptr[idx] = value; }

# æ•°ç»„æ“ä½œ
tl.arange(0, BLOCK_SIZE) â†’ ä½¿ç”¨ threadIdx.x + blockIdx.x * blockDim.x
tl.sum(x) â†’ ä½¿ç”¨reduction patternæˆ–shared memory
tl.max(x) â†’ ä½¿ç”¨reduction patternæˆ–shared memory

# å¸¸é‡
BLOCK_SIZE â†’ 256, 512, 1024 (2çš„å¹‚)
```

## ğŸ§ª DEBUGGING CHECKLIST:
- [ ] æ‰€æœ‰å˜é‡éƒ½å·²å£°æ˜
- [ ] æ‰€æœ‰æ•°ç»„è®¿é—®éƒ½æœ‰è¾¹ç•Œæ£€æŸ¥
- [ ] kernelå‡½æ•°æœ‰__global__ä¿®é¥°ç¬¦
- [ ] æ•°æ®ç±»å‹åŒ¹é… (float*, int*, etc.)
- [ ] Grid/Blockå°ºå¯¸è®¡ç®—æ­£ç¡®
- [ ] åŒ…å«å¿…è¦çš„å¤´æ–‡ä»¶
- [ ] é”™è¯¯æ£€æŸ¥ä»£ç å·²æ·»åŠ 

è¯·è½¬æ¢ä»¥ä¸‹Tritonä»£ç ï¼š

```python
"""
    
    suffix = """
```

## ğŸš€ OUTPUT REQUIREMENTS:
1. è¾“å‡ºå®Œæ•´å¯è¿è¡Œçš„Pythonä»£ç 
2. ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—æˆ–æ³¨é‡Š
3. ä»£ç å¿…é¡»é€šè¿‡nvccç¼–è¯‘
4. å¿…é¡»äº§ç”Ÿæ­£ç¡®çš„æ•°å€¼ç»“æœ
5. åŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†
6. ModelNewç±»å¿…é¡»æ­£ç¡®å®ç°

âš ï¸ å¦‚æœä¸ç¡®å®šæŸä¸ªè½¬æ¢ï¼Œé€‰æ‹©æœ€ä¿å®ˆå’Œå®‰å…¨çš„å®ç°æ–¹å¼ï¼"""
    
    return robust_prompt + triton_code + suffix

def get_simple_prompt(triton_code):
    """
    ç®€åŒ–ç‰ˆæœ¬çš„promptï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•
    """
    simple_prompt = """Convert the following Triton code to equivalent CUDA code.

Requirements:
- Must compile with nvcc
- Must produce same numerical results
- Must include ModelNew class inheriting from torch.nn.Module
- Use torch.utils.cpp_extension.load_inline

Triton code:
```python
"""
    
    suffix = """
```

Output complete Python code with CUDA implementation:"""
    
    return simple_prompt + triton_code + suffix 

def get_model_configs():
    """
    è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®
    """
    return {
        "glm-4": {
            "model": "glm-4",
            "api_key": "NaN",
            "platform": "zhipuai",
            "enabled": False,
            "description": "GLM-4 åŸºç¡€ç‰ˆæœ¬"
        },
        "glm-4-plus": {
            "model": "glm-4-plus",
            "api_key": "5bf98ea765f642aeb720420e522592f7.DWMrwJ2rfsWPYhHJ",
            "platform": "zhipuai",
            "enabled": True,
            "description": "GLM-4 å¢å¼ºç‰ˆæœ¬ï¼Œæ¨èä½¿ç”¨"
        },
        "glm-4-0520": {
            "model": "glm-4-0520",
            "api_key": "NaN",
            "platform": "zhipuai",
            "enabled": False,
            "description": "GLM-4 ç‰¹å®šç‰ˆæœ¬"
        },
        #install OpenAI SDK first: `pip3 install openai`
        "claude-sonnet-4":{
            "model": "anthropic/claude-sonnet-4",
            "api_key": "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",
            "platform": "openrouter",
            "enabled": True,
            "description" : "å®‡å®™æœ€å¼ºç¼–ç¨‹æ¨¡å‹Clude-4"
        },
        "deepseek-R1":{
            "model": "deepseek-reasoner",
            "api_key": "sk-b471c7924f5c4d3c92d3a8fc12e0150b",
            "platform": "deepseek",
            "enabled": True,
            "description" : "DeepSeek-R1-0528"
        }
    }

def list_available_models():
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
    """
    configs = get_model_configs()
    available_models = []
    for model_name, config in configs.items():
        if config.get("enabled", True):
            available_models.append({
                "name": model_name,
                "model": config["model"],
                "platform": config["platform"],
                "description": config.get("description", "")
            })
    return available_models

def create_api_client(platform, api_key):
    """
    æ ¹æ®å¹³å°åˆ›å»ºAPIå®¢æˆ·ç«¯
    """
    if platform == "zhipuai":
        return ZhipuAI(api_key=api_key)
    elif platform == "openrouter":
        # ä¸ºOpenRouter/Claudeé…ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
        return OpenAI(
            api_key=api_key,
            base_url="https://openrouter.ai/api/v1",
            timeout=120.0,  # 2åˆ†é’Ÿè¶…æ—¶
            max_retries=3   # è‡ªåŠ¨é‡è¯•3æ¬¡
        )
    elif platform == "deepseek":
        return OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com/v1",
            timeout=60.0
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å¹³å°: {platform}")


def triton2cuda(triton_code, model_type="claude-sonnet-4", prompt_type="function"):
    """
    å°†Tritonä»£ç è½¬æ¢ä¸ºCUDAä»£ç 
    
    Args:
        triton_code: Tritonæºä»£ç 
        model_type: è¦ä½¿ç”¨çš„æ¨¡å‹ç±»å‹ï¼Œé»˜è®¤ä¸º glm-4-plus
        prompt_type: promptç±»å‹ï¼Œå¯é€‰ "full", "robust", "simple"
    
    Returns:
        è½¬æ¢åçš„CUDAä»£ç 
    """
    # è·å–æ¨¡å‹é…ç½®
    model_configs = get_model_configs()
    
    # éªŒè¯æ¨¡å‹ç±»å‹æ˜¯å¦å­˜åœ¨
    if model_type not in model_configs:
        available = list(model_configs.keys())
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}ï¼Œå¯ç”¨é€‰é¡¹: {available}")
    
    selected_config = model_configs[model_type]
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
    if not selected_config.get("enabled", True):
        raise ValueError(f"æ¨¡å‹ {model_type} å·²è¢«ç¦ç”¨")
    
    # é€‰æ‹©promptç­–ç•¥
    if prompt_type == "robust":
        prompt_content = get_robust_prompt(triton_code)
    elif prompt_type == "full":
        prompt_content = get_full_prompt(triton_code)
    elif prompt_type == "simple":
        prompt_content = get_simple_prompt(triton_code)
    elif prompt_type == "function":
        prompt_content = get_prompt_through_function(triton_code)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„promptç±»å‹: {prompt_type}ï¼Œå¯é€‰: 'robust', 'full', 'simple'")
    
    # åˆ›å»ºå®¢æˆ·ç«¯å¹¶å‘é€è¯·æ±‚
    try:
        client = create_api_client(selected_config["platform"], selected_config["api_key"])
        response = client.chat.completions.create(
            model=selected_config["model"],
            messages=[
                {
                    "role": "system",
                    "content": "You are a professional Triton to CUDA conversion expert. Focus on generating error-free, compilable code.",
                },
                {"role": "user", "content": prompt_content},
            ],
            temperature=0.1,  # é™ä½éšæœºæ€§ï¼Œæé«˜ä¸€è‡´æ€§
            max_tokens=4000,  # ç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´ç”Ÿæˆå®Œæ•´ä»£ç 
        )
        content = response.choices[0].message.content
    except Exception as e:
        raise RuntimeError(f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
    
    # æå–ä»£ç å—ä¸­çš„å†…å®¹
    if "```python" in content:
        start = content.find("```python") + len("```python")
        end = content.find("```", start)
        if end != -1:
            return content[start:end].strip()
    elif "```" in content:
        start = content.find("```") + 3
        end = content.find("```", start)
        if end != -1:
            return content[start:end].strip()

    return content
