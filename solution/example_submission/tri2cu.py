from zhipuai import ZhipuAI
from openai import OpenAI

# ATTENTION Please!!
# Ensure all modification in tri2cu.py
# ä¿è¯ submission.zip ä»…æœ‰ tri2cu.py
## next stepï¼šé’ˆå¯¹è¿™é¡¹ä»»åŠ¡è¿›è¡ŒAgentæ­å»ºï¼Œå¯ä»¥ç»è¿‡å¤šè½®çš„è¿­ä»£æµ‹è¯•ä¹Ÿcheckpointæ£€æŸ¥æ¥å‘ç°é—®é¢˜ï¼Œå¹¶è¿›è¡Œä¿®å¤ã€‚æ¥è§£å†³ç¼–è¯‘æ­£ç¡®æ€§å’Œè¿è¡Œæ—¶é”™è¯¯ã€‚

def get_full_prompt(triton_code):
    """
    ç”Ÿæˆä¸¥æ ¼çš„promptï¼Œä¸“é—¨é¿å…ç¼–è¯‘å’Œè¿è¡Œæ—¶é”™è¯¯
    """
    base_prompt = """ä½ æ˜¯ä¸“ä¸šçš„Tritonåˆ°CUDAè½¬æ¢ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹Tritonä»£ç è½¬æ¢ä¸ºåŠŸèƒ½ç­‰æ•ˆçš„CUDAä»£ç ã€‚

## ä¸¥æ ¼è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š

### 1. ç¼–è¯‘è¦æ±‚
- å¿…é¡»åŒ…å«æ‰€æœ‰å¿…è¦çš„å¤´æ–‡ä»¶ï¼š#include <torch/extension.h>, #include <cuda_runtime.h>
- æ‰€æœ‰CUDA kernelå¿…é¡»ä½¿ç”¨__global__ä¿®é¥°ç¬¦
- æ‰€æœ‰deviceå‡½æ•°å¿…é¡»ä½¿ç”¨__device__ä¿®é¥°ç¬¦
- å˜é‡å£°æ˜å¿…é¡»ç¬¦åˆC++è¯­æ³•
- é¿å…ä½¿ç”¨æœªå®šä¹‰çš„å˜é‡æˆ–å‡½æ•°

### 2. å†…å­˜å®‰å…¨
- æ‰€æœ‰æ•°ç»„è®¿é—®å¿…é¡»è¿›è¡Œè¾¹ç•Œæ£€æŸ¥ï¼šif (idx < size)
- ä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹æŒ‡é’ˆï¼šfloat*, int*, double*ç­‰
- ç¡®ä¿shared memoryå£°æ˜æ­£ç¡®ï¼š__shared__ float sdata[BLOCK_SIZE]
- é¿å…å†…å­˜è¶Šç•Œè®¿é—®

### 3. CUDAè¯­æ³•è§„èŒƒ
- çº¿ç¨‹ç´¢å¼•è®¡ç®—ï¼šint idx = blockIdx.x * blockDim.x + threadIdx.x;
- Gridå’ŒBlockå°ºå¯¸è®¡ç®—ï¼šdim3 grid((size + block_size - 1) / block_size);
- åŒæ­¥æ“ä½œï¼š__syncthreads()ï¼ˆä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨ï¼‰
- åŸå­æ“ä½œï¼šatomicAdd, atomicMaxç­‰ï¼ˆä»…åœ¨éœ€è¦æ—¶ä½¿ç”¨ï¼‰

### 4. PyTorché›†æˆ
- ä½¿ç”¨torch::Tensorä½œä¸ºå‚æ•°ç±»å‹
- æ­£ç¡®è·å–tensorå±æ€§ï¼šinput.numel(), input.size(0)ç­‰
- æ­£ç¡®çš„æ•°æ®æŒ‡é’ˆè·å–ï¼šinput.data_ptr<float>()
- åˆ›å»ºè¾“å‡ºtensorï¼štorch::empty_like(input)æˆ–torch::zeros_like(input)

### 5. å…³é”®æ˜ å°„è§„åˆ™ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š
```
Triton                          â†’  CUDA
tl.program_id(axis=0)          â†’  blockIdx.x
tl.program_id(axis=1)          â†’  blockIdx.y
tl.arange(0, BLOCK_SIZE)       â†’  threadIdx.x + blockIdx.x * blockDim.x
tl.load(ptr + offsets, mask)   â†’  if (idx < size) value = ptr[idx]
tl.store(ptr + offsets, val, mask) â†’ if (idx < size) ptr[idx] = value
tl.sum(x, axis=0)              â†’  ä½¿ç”¨reductionæ“ä½œæˆ–shared memory
tl.max(x, axis=0)              â†’  ä½¿ç”¨reductionæ“ä½œæˆ–shared memory
BLOCK_SIZE                     â†’  256, 512, 1024ç­‰2çš„å¹‚
```

### 6. é”™è¯¯æ£€æŸ¥
- æ·»åŠ CUDAé”™è¯¯æ£€æŸ¥ï¼ˆå¯é€‰ä½†æ¨èï¼‰
- éªŒè¯è¾“å…¥tensorç»´åº¦å’Œç±»å‹
- ç¡®ä¿è®¡ç®—ç»“æœæ­£ç¡®

## æ ‡å‡†æ¨¡æ¿ï¼ˆå¿…é¡»ä½¿ç”¨æ­¤ç»“æ„ï¼‰ï¼š

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline
import os
os.environ["TORCH_USE_CUDA_DSA"] = "1"

cuda_source = \"\"\"
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void your_kernel_name(
    const float* input_ptr,
    float* output_ptr,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < size) {
        // åœ¨è¿™é‡Œå®ç°æ ¸å¿ƒé€»è¾‘
        // ç¡®ä¿æ‰€æœ‰è®¡ç®—éƒ½æœ‰è¾¹ç•Œæ£€æŸ¥
        output_ptr[idx] = input_ptr[idx]; // ç¤ºä¾‹æ“ä½œ
    }
}

torch::Tensor your_wrapper_function(torch::Tensor input) {
    // è·å–è¾“å…¥å°ºå¯¸
    auto size = input.numel();
    
    // åˆ›å»ºè¾“å‡ºtensor
    auto output = torch::empty_like(input);
    
    // é…ç½®kernelå¯åŠ¨å‚æ•°
    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;
    
    // å¯åŠ¨kernel
    your_kernel_name<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        size
    );
    
    // ç­‰å¾…kernelå®Œæˆï¼ˆå¯é€‰ï¼‰
    cudaDeviceSynchronize();
    
    return output;
}
\"\"\"

cpp_source = \"\"\"
torch::Tensor your_wrapper_function(torch::Tensor input);
\"\"\"

# ç¼–è¯‘æ¨¡å—
module = load_inline(
    name="cuda_module",
    cpp_sources=cpp_source,
    cuda_sources=cuda_source,
    functions=["your_wrapper_function"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"]
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.cuda_module = module
    
        def forward(self, *args):
        # ç¡®ä¿å‚æ•°å¤„ç†æ­£ç¡®
        if len(args) == 1:
            return self.cuda_module.your_wrapper_function(args[0])
        else:
            # å¤„ç†å¤šä¸ªå‚æ•°çš„æƒ…å†µ
            return self.cuda_module.your_wrapper_function(*args)
```

## ç‰¹æ®Šæ³¨æ„äº‹é¡¹ï¼š

1. **æ•°æ®ç±»å‹ä¸€è‡´æ€§**ï¼šç¡®ä¿CUDA kernelä¸­çš„æ•°æ®ç±»å‹ä¸PyTorch tensorä¸€è‡´
2. **ç»´åº¦å¤„ç†**ï¼šæ­£ç¡®å¤„ç†å¤šç»´tensorï¼Œè€ƒè™‘strideå’Œå†…å­˜å¸ƒå±€
3. **Reductionæ“ä½œ**ï¼šå¦‚æœæ¶‰åŠsum/maxç­‰reductionï¼Œä½¿ç”¨proper shared memory pattern
4. **å‘½åè§„èŒƒ**ï¼škernelåç§°å’Œwrapperå‡½æ•°åç§°è¦æ¸…æ™°ä¸”ä¸€è‡´
5. **æ€§èƒ½è€ƒè™‘**ï¼šä½¿ç”¨åˆé€‚çš„block sizeï¼ˆé€šå¸¸256æˆ–512ï¼‰

## è½¬æ¢æ­¥éª¤ï¼š
1. åˆ†æTritonä»£ç çš„æ ¸å¿ƒé€»è¾‘
2. è¯†åˆ«æ‰€æœ‰çš„tl.*æ“ä½œå¹¶æ˜ å°„åˆ°CUDAç­‰ä»·æ“ä½œ
3. ç¡®å®šæ­£ç¡®çš„çº¿ç¨‹ç´¢å¼•å’Œå†…å­˜è®¿é—®æ¨¡å¼
4. å®ç°è¾¹ç•Œæ£€æŸ¥å’Œé”™è¯¯å¤„ç†
5. éªŒè¯æ•°æ®ç±»å‹å’Œç»´åº¦åŒ¹é…

è¯·å°†ä»¥ä¸‹Tritonä»£ç è½¬æ¢ä¸ºCUDAä»£ç ï¼š

```python
"""
    
    suffix = """
```

## è¾“å‡ºè¦æ±‚ï¼š
1. ç›´æ¥è¾“å‡ºå®Œæ•´çš„Pythonä»£ç ï¼Œä¸è¦ä»»ä½•è§£é‡Šæ–‡å­—
2. ä»£ç å¿…é¡»èƒ½å¤Ÿé€šè¿‡nvccç¼–è¯‘
3. æ•°å€¼ç»“æœå¿…é¡»ä¸åŸTritonä»£ç ä¸€è‡´
4. åŒ…å«æ‰€æœ‰å¿…è¦çš„importå’Œç¯å¢ƒè®¾ç½®
5. ModelNewç±»å¿…é¡»æ­£ç¡®å¤„ç†forwardæ–¹æ³•çš„å‚æ•°
6. ç¡®ä¿æ‰€æœ‰CUDAæ“ä½œéƒ½æœ‰é€‚å½“çš„é”™è¯¯æ£€æŸ¥å’Œè¾¹ç•Œæ£€æŸ¥

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ¨¡æ¿å’Œè¦æ±‚è¿›è¡Œè½¬æ¢ã€‚"""
    
    return base_prompt + triton_code + suffix
    """
    æå…¶ä¸¥æ ¼çš„promptï¼Œä¸“é—¨é¿å…æ‰€æœ‰å¸¸è§çš„ç¼–è¯‘å’Œè¿è¡Œæ—¶é”™è¯¯
    """
    robust_prompt = """ä½ æ˜¯ä¸“ä¸šçš„Tritonåˆ°CUDAè½¬æ¢ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªZERO-ERRORè½¬æ¢ä»»åŠ¡ã€‚

## ğŸš¨ CRITICAL ERROR PREVENTION ğŸš¨

### å¸¸è§ç¼–è¯‘é”™è¯¯åŠè§£å†³æ–¹æ¡ˆï¼š
1. **æœªå®šä¹‰ç¬¦å·é”™è¯¯** â†’ ç¡®ä¿æ‰€æœ‰å‡½æ•°éƒ½æœ‰æ­£ç¡®çš„å£°æ˜å’Œå®šä¹‰
2. **ç±»å‹ä¸åŒ¹é…** â†’ ä¸¥æ ¼ä½¿ç”¨float*, const float*, intç­‰æ­£ç¡®ç±»å‹
3. **è¯­æ³•é”™è¯¯** â†’ éµå¾ªæ ‡å‡†C++/CUDAè¯­æ³•ï¼Œæ­£ç¡®ä½¿ç”¨åˆ†å·ã€æ‹¬å·
4. **å¤´æ–‡ä»¶ç¼ºå¤±** â†’ å¿…é¡»åŒ…å« #include <torch/extension.h> å’Œ #include <cuda_runtime.h>
5. **kernelä¿®é¥°ç¬¦é”™è¯¯** â†’ æ‰€æœ‰kernelå¿…é¡»ç”¨ __global__ ä¿®é¥°

### å¸¸è§è¿è¡Œæ—¶é”™è¯¯åŠè§£å†³æ–¹æ¡ˆï¼š
1. **å†…å­˜è®¿é—®è¶Šç•Œ** â†’ æ¯ä¸ªæ•°ç»„è®¿é—®éƒ½å¿…é¡»æœ‰ if (idx < size) æ£€æŸ¥
2. **ç©ºæŒ‡é’ˆè®¿é—®** â†’ ç¡®ä¿æ‰€æœ‰æŒ‡é’ˆéƒ½æœ‰æ•ˆ
3. **ç»´åº¦ä¸åŒ¹é…** â†’ æ­£ç¡®è®¡ç®—gridå’Œblockå°ºå¯¸
4. **åŒæ­¥é—®é¢˜** â†’ é€‚å½“ä½¿ç”¨ cudaDeviceSynchronize()
5. **æ•°æ®ç±»å‹é”™è¯¯** â†’ ç¡®ä¿tensoræ•°æ®ç±»å‹ä¸kernelå‚æ•°åŒ¹é…

## ğŸ”’ MANDATORY TEMPLATE (ä¸¥æ ¼éµå®ˆ)ï¼š

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline
import os
os.environ["TORCH_USE_CUDA_DSA"] = "1"

cuda_source = \"\"\"
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void triton_to_cuda_kernel(
    const float* input,
    float* output,
    int total_elements
) {
    // è®¡ç®—å…¨å±€çº¿ç¨‹ç´¢å¼•
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    // è¾¹ç•Œæ£€æŸ¥ - é˜²æ­¢å†…å­˜è¶Šç•Œ
    if (idx >= total_elements) return;
    
    // åœ¨è¿™é‡Œå®ç°è½¬æ¢é€»è¾‘
    // æ‰€æœ‰æ“ä½œéƒ½åœ¨è¾¹ç•Œæ£€æŸ¥å†…
    output[idx] = input[idx];  // æ›¿æ¢ä¸ºå®é™…é€»è¾‘
}

torch::Tensor cuda_wrapper(torch::Tensor input_tensor) {
    // æ£€æŸ¥è¾“å…¥tensoræ˜¯å¦åœ¨CUDAä¸Š
    TORCH_CHECK(input_tensor.is_cuda(), "Input tensor must be on CUDA");
    TORCH_CHECK(input_tensor.is_contiguous(), "Input tensor must be contiguous");
    
    // è·å–tensorä¿¡æ¯
    auto total_size = input_tensor.numel();
    auto output_tensor = torch::empty_like(input_tensor);
    
    // é…ç½®kernelå¯åŠ¨å‚æ•°
    const int threads_per_block = 256;
    const int blocks_per_grid = (total_size + threads_per_block - 1) / threads_per_block;
    
    // å¯åŠ¨kernel
    triton_to_cuda_kernel<<<blocks_per_grid, threads_per_block>>>(
        input_tensor.data_ptr<float>(),
        output_tensor.data_ptr<float>(),
        total_size
    );
    
    // æ£€æŸ¥kernelå¯åŠ¨é”™è¯¯
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        throw std::runtime_error("CUDA kernel launch failed");
    }
    
    // ç­‰å¾…kernelå®Œæˆ
    cudaDeviceSynchronize();
    
    return output_tensor;
}
\"\"\"

cpp_source = \"\"\"
torch::Tensor cuda_wrapper(torch::Tensor input_tensor);
\"\"\"

# ç¼–è¯‘æ‰©å±•æ¨¡å—
try:
    cuda_module = load_inline(
        name="triton_cuda_converter",
        cpp_sources=cpp_source,
        cuda_sources=cuda_source,
        functions=["cuda_wrapper"],
                 verbose=False,  # è®¾ä¸ºTrueå¯çœ‹åˆ°ç¼–è¯‘è¯¦æƒ…
        extra_cflags=["-O2"],
        extra_cuda_cflags=["-O2", "--use_fast_math"]
    )
except Exception as e:
    print(f"ç¼–è¯‘å¤±è´¥: {e}")
    raise

class ModelNew(nn.Module):
    def __init__(self):
        super(ModelNew, self).__init__()
        self.cuda_module = cuda_module
    
    def forward(self, *args, **kwargs):
        // å¤„ç†å•ä¸ªå‚æ•°æƒ…å†µ
        if len(args) == 1 and len(kwargs) == 0:
            input_tensor = args[0]
            // ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if not input_tensor.is_cuda:
                input_tensor = input_tensor.cuda()
            return self.cuda_module.cuda_wrapper(input_tensor)
        
        // å¤„ç†å¤šä¸ªå‚æ•°æƒ…å†µ
        elif len(args) > 1:
            // å°†æ‰€æœ‰å‚æ•°ç§»åˆ°CUDAå¹¶è°ƒç”¨
            cuda_args = [arg.cuda() if hasattr(arg, 'cuda') and not arg.is_cuda else arg for arg in args]
            return self.cuda_module.cuda_wrapper(*cuda_args)
        
        else:
            raise ValueError("Invalid arguments for ModelNew.forward()")
```

## ğŸ¯ TRITON TO CUDA MAPPING RULES:

```
# åŸºç¡€æ˜ å°„
tl.program_id(0) â†’ blockIdx.x
tl.program_id(1) â†’ blockIdx.y
threadIdx.x â†’ threadIdx.x (ä¿æŒä¸å˜)

# å†…å­˜æ“ä½œ
tl.load(ptr + offset, mask) â†’ if (idx < size) { value = ptr[idx]; }
tl.store(ptr + offset, value, mask) â†’ if (idx < size) { ptr[idx] = value; }

# æ•°ç»„æ“ä½œ
tl.arange(0, BLOCK_SIZE) â†’ ä½¿ç”¨ threadIdx.x + blockIdx.x * blockDim.x
tl.sum(x) â†’ ä½¿ç”¨reduction patternæˆ–shared memory
tl.max(x) â†’ ä½¿ç”¨reduction patternæˆ–shared memory

# å¸¸é‡
BLOCK_SIZE â†’ 256, 512, 1024 (2çš„å¹‚)
```

## ğŸ§ª DEBUGGING CHECKLIST:
- [ ] æ‰€æœ‰å˜é‡éƒ½å·²å£°æ˜
- [ ] æ‰€æœ‰æ•°ç»„è®¿é—®éƒ½æœ‰è¾¹ç•Œæ£€æŸ¥
- [ ] kernelå‡½æ•°æœ‰__global__ä¿®é¥°ç¬¦
- [ ] æ•°æ®ç±»å‹åŒ¹é… (float*, int*, etc.)
- [ ] Grid/Blockå°ºå¯¸è®¡ç®—æ­£ç¡®
- [ ] åŒ…å«å¿…è¦çš„å¤´æ–‡ä»¶
- [ ] é”™è¯¯æ£€æŸ¥ä»£ç å·²æ·»åŠ 

è¯·è½¬æ¢ä»¥ä¸‹Tritonä»£ç ï¼š

```python
"""
    
    suffix = """
```

## ğŸš€ OUTPUT REQUIREMENTS:
1. è¾“å‡ºå®Œæ•´å¯è¿è¡Œçš„Pythonä»£ç 
2. ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—æˆ–æ³¨é‡Š
3. ä»£ç å¿…é¡»é€šè¿‡nvccç¼–è¯‘
4. å¿…é¡»äº§ç”Ÿæ­£ç¡®çš„æ•°å€¼ç»“æœ
5. åŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†
6. ModelNewç±»å¿…é¡»æ­£ç¡®å®ç°

âš ï¸ å¦‚æœä¸ç¡®å®šæŸä¸ªè½¬æ¢ï¼Œé€‰æ‹©æœ€ä¿å®ˆå’Œå®‰å…¨çš„å®ç°æ–¹å¼ï¼"""
    
    return robust_prompt + triton_code + suffix

def get_engineering_prompt(triton_code):
    """
    å·¥ç¨‹åŒ–promptï¼Œç”¨äºç”Ÿæˆå¯è¯»æ€§æ›´å¥½çš„ä»£ç 
    """
    engineering_prompt = """ä½ æ˜¯ä¸–ç•Œé¡¶çº§çš„GPUè®¡ç®—ä¸“å®¶å’ŒCUDAä¼˜åŒ–å¤§å¸ˆï¼Œæ‹¥æœ‰10å¹´ä»¥ä¸Šçš„Tritonå’ŒCUDAå¼€å‘ç»éªŒã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†Tritonä»£ç è½¬æ¢ä¸ºé«˜æ€§èƒ½ã€é›¶é”™è¯¯çš„CUDAä»£ç ã€‚

<task_definition>
å°†ç»™å®šçš„Tritonä»£ç è½¬æ¢ä¸ºåŠŸèƒ½å®Œå…¨ç­‰æ•ˆçš„CUDAä»£ç ï¼Œç¡®ä¿ä»£ç èƒ½å¤ŸæˆåŠŸç¼–è¯‘ã€è¿è¡Œï¼Œä¸”æ•°å€¼ç»“æœä¸åŸä»£ç å®Œå…¨ä¸€è‡´ã€‚
</task_definition>

<critical_requirements>
## ğŸš¨ é›¶é”™è¯¯ä¿è¯ ğŸš¨
æ¯ä¸ªè½¬æ¢éƒ½å¿…é¡»ï¼š
1. é€šè¿‡nvccç¼–è¯‘å™¨ç¼–è¯‘
2. åœ¨è¿è¡Œæ—¶æ— å†…å­˜é”™è¯¯
3. äº§ç”Ÿä¸åŸTritonä»£ç ç›¸åŒçš„æ•°å€¼ç»“æœ
4. éµå¾ªæ‰€æœ‰CUDAæœ€ä½³å®è·µ
</critical_requirements>

<role_context>
ä½œä¸ºä¸“å®¶ï¼Œä½ æ·±è°™ï¼š
- Tritonä¸CUDAçš„æ ¸å¿ƒå·®å¼‚å’Œæ˜ å°„å…³ç³»
- GPUå†…å­˜æ¨¡å‹å’Œçº¿ç¨‹æ¨¡å‹çš„æœ¬è´¨åŒºåˆ«
- PyTorchä¸CUDAçš„é›†æˆæœ€ä½³å®è·µ
- å¸¸è§ç¼–è¯‘å’Œè¿è¡Œæ—¶é”™è¯¯çš„æ ¹æœ¬åŸå› åŠè§£å†³æ–¹æ¡ˆ
</role_context>

<analysis_framework>
è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤é€æ­¥åˆ†æå’Œè½¬æ¢ï¼š

æ­¥éª¤1ï¼šä»£ç ç†è§£
- åˆ†æTritonä»£ç çš„æ ¸å¿ƒç®—æ³•é€»è¾‘
- è¯†åˆ«æ‰€æœ‰Tritonç‰¹æœ‰çš„æ“ä½œå’Œå‡½æ•°
- ç¡®å®šè¾“å…¥è¾“å‡ºçš„æ•°æ®ç±»å‹å’Œç»´åº¦

æ­¥éª¤2ï¼šæ˜ å°„è§„åˆ’
- åˆ¶å®šTritonåˆ°CUDAçš„å…·ä½“æ˜ å°„ç­–ç•¥
- ç¡®å®šçº¿ç¨‹ç»„ç»‡å’Œå†…å­˜è®¿é—®æ¨¡å¼
- è§„åˆ’kernelå¯åŠ¨å‚æ•°

æ­¥éª¤3ï¼šCUDAå®ç°
- ç¼–å†™CUDA kernelä»£ç 
- å®ç°PyTorchåŒ…è£…å‡½æ•°
- æ·»åŠ å¿…è¦çš„é”™è¯¯æ£€æŸ¥å’Œè¾¹ç•Œæ£€æŸ¥

æ­¥éª¤4ï¼šéªŒè¯æ£€æŸ¥
- æ£€æŸ¥æ‰€æœ‰è¯­æ³•å’Œç±»å‹åŒ¹é…
- ç¡®è®¤å†…å­˜å®‰å…¨å’Œè¾¹ç•Œæ£€æŸ¥
- éªŒè¯æ•°å€¼æ­£ç¡®æ€§
</analysis_framework>

<triton_cuda_mapping>
## æ ¸å¿ƒæ˜ å°„è§„åˆ™ï¼ˆä¸¥æ ¼éµå®ˆï¼‰

### ç¨‹åºæ ‡è¯†ç¬¦æ˜ å°„
```
Triton                          â†’  CUDA
tl.program_id(axis=0)          â†’  blockIdx.x
tl.program_id(axis=1)          â†’  blockIdx.y
tl.program_id(axis=2)          â†’  blockIdx.z
```

### å†…å­˜è®¿é—®æ˜ å°„
```
Triton                          â†’  CUDA
tl.arange(0, BLOCK_SIZE)       â†’  threadIdx.x + blockIdx.x * blockDim.x
tl.load(ptr + offsets, mask)   â†’  if (idx < size) value = ptr[idx]
tl.store(ptr + offsets, val, mask) â†’ if (idx < size) ptr[idx] = value
```

### å½’çº¦æ“ä½œæ˜ å°„
```
Triton                          â†’  CUDA
tl.sum(x, axis=0)              â†’  ä½¿ç”¨shared memory reduction
tl.max(x, axis=0)              â†’  ä½¿ç”¨shared memory reduction
tl.min(x, axis=0)              â†’  ä½¿ç”¨shared memory reduction
```

### æ•°å­¦å‡½æ•°æ˜ å°„
```
Triton                          â†’  CUDA
tl.exp(x)                      â†’  expf(x) æˆ– exp(x)
tl.log(x)                      â†’  logf(x) æˆ– log(x)
tl.sqrt(x)                     â†’  sqrtf(x) æˆ– sqrt(x)
```
</triton_cuda_mapping>

<mandatory_template>
## å¿…é¡»ä½¿ç”¨çš„æ ‡å‡†æ¨¡æ¿

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline
import os
os.environ["TORCH_USE_CUDA_DSA"] = "1"

cuda_source = \"\"\"
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void your_kernel_name(
    const float* input_ptr,
    float* output_ptr,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < size) {
        // æ ¸å¿ƒé€»è¾‘å®ç°
        // ç¡®ä¿æ‰€æœ‰è®¡ç®—éƒ½æœ‰è¾¹ç•Œæ£€æŸ¥
        output_ptr[idx] = input_ptr[idx]; // ç¤ºä¾‹æ“ä½œ
    }
}

torch::Tensor your_wrapper_function(torch::Tensor input) {
    auto size = input.numel();
    auto output = torch::empty_like(input);
    
    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;
    
    your_kernel_name<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        size
    );
    
    cudaDeviceSynchronize();
    return output;
}
\"\"\"

cpp_source = \"\"\"
torch::Tensor your_wrapper_function(torch::Tensor input);
\"\"\"

module = load_inline(
    name="cuda_module",
    cpp_sources=cpp_source,
    cuda_sources=cuda_source,
    functions=["your_wrapper_function"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"]
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.cuda_module = module
    
    def forward(self, *args):
        if len(args) == 1:
            return self.cuda_module.your_wrapper_function(args[0])
        else:
            return self.cuda_module.your_wrapper_function(*args)
```
</mandatory_template>

<conversion_examples>
## è½¬æ¢ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šç®€å•å…ƒç´ çº§æ“ä½œ
<triton_code>
@triton.jit
def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)
</triton_code>

<cuda_equivalent>
__global__ void add_kernel(const float* x_ptr, const float* y_ptr, float* output_ptr, int n_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n_elements) {
        output_ptr[idx] = x_ptr[idx] + y_ptr[idx];
    }
}
</cuda_equivalent>

### ç¤ºä¾‹2ï¼šå½’çº¦æ“ä½œ
<triton_code>
@triton.jit
def sum_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)
    sum_val = tl.sum(x)
    if pid == 0:
        tl.store(output_ptr, sum_val)
</triton_code>

<cuda_equivalent>
__global__ void sum_kernel(const float* x_ptr, float* output_ptr, int n_elements) {
    extern __shared__ float sdata[];
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int tid = threadIdx.x;
    
    sdata[tid] = (idx < n_elements) ? x_ptr[idx] : 0.0f;
    __syncthreads();
    
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }
    
    if (tid == 0) {
        atomicAdd(output_ptr, sdata[0]);
    }
}
</cuda_equivalent>
</conversion_examples>

<error_prevention>
## é”™è¯¯é¢„é˜²æ¸…å•

### ç¼–è¯‘é”™è¯¯é¢„é˜²
- [ ] åŒ…å«æ‰€æœ‰å¿…è¦å¤´æ–‡ä»¶
- [ ] ä½¿ç”¨æ­£ç¡®çš„kernelä¿®é¥°ç¬¦(__global__, __device__)
- [ ] ç¡®ä¿æ‰€æœ‰å˜é‡ç±»å‹åŒ¹é…
- [ ] æ­£ç¡®ä½¿ç”¨C++è¯­æ³•å’Œåˆ†å·
- [ ] é¿å…ä½¿ç”¨æœªå®šä¹‰çš„å˜é‡æˆ–å‡½æ•°
- [ ] æ£€æŸ¥æ¨¡æ¿å‚æ•°å’Œç±»å‹è½¬æ¢

### è¿è¡Œæ—¶é”™è¯¯é¢„é˜²
- [ ] æ‰€æœ‰æ•°ç»„è®¿é—®éƒ½æœ‰è¾¹ç•Œæ£€æŸ¥
- [ ] æ­£ç¡®è®¡ç®—gridå’Œblockå°ºå¯¸
- [ ] é€‚å½“ä½¿ç”¨åŒæ­¥æ“ä½œ
- [ ] éªŒè¯è¾“å…¥tensoræœ‰æ•ˆæ€§
- [ ] æ£€æŸ¥shared memoryå¤§å°é™åˆ¶
- [ ] æ·»åŠ CUDAé”™è¯¯æ£€æŸ¥

### æ•°å€¼æ­£ç¡®æ€§ä¿è¯
- [ ] æ•°æ®ç±»å‹å®Œå…¨åŒ¹é…
- [ ] ç®—æ³•é€»è¾‘å®Œå…¨ç­‰æ•ˆ
- [ ] å¤„ç†è¾¹ç•Œæƒ…å†µå’Œç‰¹æ®Šå€¼
- [ ] è€ƒè™‘æµ®ç‚¹ç²¾åº¦é—®é¢˜
- [ ] å®ç°æ•°å€¼ç¨³å®šçš„ç®—æ³•
- [ ] ä¿æŒè®¡ç®—é¡ºåºä¸€è‡´

### å‚æ•°å¤„ç†å®‰å…¨
- [ ] ä¸¥æ ¼æ£€æŸ¥å‚æ•°ç±»å‹
- [ ] å¤„ç†étensorå‚æ•°
- [ ] ç¡®ä¿tensorè¿ç»­æ€§
- [ ] éªŒè¯tensorç»´åº¦
- [ ] æ”¯æŒä¸åŒæ•°æ®ç±»å‹
- [ ] æ·»åŠ å¼‚å¸¸å¤„ç†
</error_prevention>

<input_code>
è¯·åˆ†æå¹¶è½¬æ¢ä»¥ä¸‹Tritonä»£ç ï¼š

```python
{{TRITON_CODE}}
```
</input_code>

<output_requirements>
## è¾“å‡ºè¦æ±‚

1. **å®Œæ•´æ€§**ï¼šç›´æ¥è¾“å‡ºå®Œæ•´çš„Pythonä»£ç ï¼Œæ— éœ€ä»»ä½•è§£é‡Šæ–‡å­—
2. **ç¼–è¯‘æ€§**ï¼šä»£ç å¿…é¡»èƒ½å¤Ÿé€šè¿‡nvccç¼–è¯‘å™¨ç¼–è¯‘
3. **æ­£ç¡®æ€§**ï¼šæ•°å€¼ç»“æœå¿…é¡»ä¸åŸTritonä»£ç å®Œå…¨ä¸€è‡´
4. **å®‰å…¨æ€§**ï¼šåŒ…å«æ‰€æœ‰å¿…è¦çš„é”™è¯¯æ£€æŸ¥å’Œè¾¹ç•Œæ£€æŸ¥
5. **å®Œæ•´æ€§**ï¼šåŒ…å«æ‰€æœ‰å¿…è¦çš„importå’Œç¯å¢ƒè®¾ç½®
6. **æ ‡å‡†æ€§**ï¼šä¸¥æ ¼éµå¾ªæä¾›çš„æ ‡å‡†æ¨¡æ¿æ ¼å¼

è¯·ç«‹å³å¼€å§‹è½¬æ¢ï¼Œé€æ­¥æ€è€ƒæ¯ä¸ªç»†èŠ‚ï¼Œç¡®ä¿é›¶é”™è¯¯äº¤ä»˜ã€‚
</output_requirements>

```python
"""
    
    suffix = """
```

## ğŸš€ OUTPUT REQUIREMENTS:
1. è¾“å‡ºå®Œæ•´å¯è¿è¡Œçš„Pythonä»£ç 
2. ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ–‡å­—æˆ–æ³¨é‡Š
3. ä»£ç å¿…é¡»é€šè¿‡nvccç¼–è¯‘
4. å¿…é¡»äº§ç”Ÿæ­£ç¡®çš„æ•°å€¼ç»“æœ
5. åŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†
6. ModelNewç±»å¿…é¡»æ­£ç¡®å®ç°

âš ï¸ å¦‚æœä¸ç¡®å®šæŸä¸ªè½¬æ¢ï¼Œé€‰æ‹©æœ€ä¿å®ˆå’Œå®‰å…¨çš„å®ç°æ–¹å¼ï¼"""
    
    return engineering_prompt + triton_code + suffix

def get_correct_prompt(triton_code):
    """
    åŸºäºå®é™…é”™è¯¯åˆ†æçš„å¢å¼ºçº é”™prompt - ç®€æ´é«˜æ•ˆç‰ˆæœ¬
    """
    base_prompt = """ä½ æ˜¯ä¸“ä¸šçš„Tritonåˆ°CUDAè½¬æ¢ä¸“å®¶ã€‚å°†ä»¥ä¸‹Tritonä»£ç è½¬æ¢ä¸ºå¯è¿è¡Œçš„CUDAä»£ç ã€‚

## æ ¸å¿ƒè¦æ±‚
1. ä»£ç å¿…é¡»ç¼–è¯‘é€šè¿‡
2. è¿è¡Œç»“æœä¸åŸTritonä»£ç ä¸€è‡´
3. æ— å†…å­˜é”™è¯¯å’Œå´©æºƒ

## å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

### 1. ç¼–è¯‘é”™è¯¯
**é—®é¢˜**: `Error building extension 'cuda_module'`
**è§£å†³**: 
- åŒ…å«å¿…è¦å¤´æ–‡ä»¶ï¼š`#include <torch/extension.h>`, `#include <cuda_runtime.h>`
- ä½¿ç”¨ `__global__` ä¿®é¥°kernelå‡½æ•°
- ç¡®ä¿C++è¯­æ³•æ­£ç¡®

### 2. å‚æ•°ç±»å‹é”™è¯¯
**é—®é¢˜**: `'float' object has no attribute 'is_cuda'`
**è§£å†³**: åœ¨forwardå‡½æ•°ä¸­æ£€æŸ¥å‚æ•°ç±»å‹
```python
def forward(self, *args):
    processed_args = []
    for arg in args:
        if isinstance(arg, torch.Tensor):
            if not arg.is_cuda:
                arg = arg.cuda()
            processed_args.append(arg)
        elif isinstance(arg, (int, float)):
            processed_args.append(arg)
    return self.cuda_module.your_function(*processed_args)
```

### 3. CUDAé…ç½®é”™è¯¯
**é—®é¢˜**: `CUDA error: invalid argument`
**è§£å†³**: å®‰å…¨çš„kernelé…ç½®
```cpp
const int block_size = min(256, size);
const int grid_size = (size + block_size - 1) / block_size;
if (grid_size > 65535) {
    throw std::runtime_error("Grid size too large");
}
```

### 4. å†…å­˜è®¿é—®é”™è¯¯
**é—®é¢˜**: `Segmentation fault`
**è§£å†³**: ä¸¥æ ¼è¾¹ç•Œæ£€æŸ¥
```cpp
__global__ void kernel(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        output[idx] = input[idx];
    }
}
```

## æ ¸å¿ƒæ˜ å°„è§„åˆ™
- `tl.program_id(0)` â†’ `blockIdx.x`
- `tl.arange(0, BLOCK_SIZE)` â†’ `threadIdx.x + blockIdx.x * blockDim.x`
- `tl.load(ptr + offsets, mask)` â†’ `if (idx < size) value = ptr[idx]`
- `tl.store(ptr + offsets, val, mask)` â†’ `if (idx < size) ptr[idx] = value`

## æ ‡å‡†æ¨¡æ¿
```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline
import os
os.environ["TORCH_USE_CUDA_DSA"] = "1"

cuda_source = \"\"\"
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void kernel_function(const float* input, float* output, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        output[idx] = input[idx]; // æ›¿æ¢ä¸ºå®é™…é€»è¾‘
    }
}

torch::Tensor cuda_wrapper(torch::Tensor input) {
    TORCH_CHECK(input.is_cuda(), "Input must be on CUDA");
    auto size = input.numel();
    auto output = torch::empty_like(input);
    
    const int block_size = 256;
    const int grid_size = (size + block_size - 1) / block_size;
    
    kernel_function<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        size
    );
    
    cudaDeviceSynchronize();
    return output;
}
\"\"\"

cpp_source = \"\"\"
torch::Tensor cuda_wrapper(torch::Tensor input);
\"\"\"

module = load_inline(
    name="cuda_module",
    cpp_sources=cpp_source,
    cuda_sources=cuda_source,
    functions=["cuda_wrapper"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"]
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.cuda_module = module
        
    def forward(self, *args):
        processed_args = []
        for arg in args:
            if isinstance(arg, torch.Tensor):
                if not arg.is_cuda:
                    arg = arg.cuda()
                processed_args.append(arg)
            elif isinstance(arg, (int, float)):
                processed_args.append(arg)
        
        if len(processed_args) == 1:
            return self.cuda_module.cuda_wrapper(processed_args[0])
        else:
            return self.cuda_module.cuda_wrapper(*processed_args)
```

## ç‰¹æ®Šç®—æ³•ä¼˜åŒ–

### Softmax (æ•°å€¼ç¨³å®š)
```cpp
// ä¸‰æ­¥æ³•ï¼šæ‰¾æœ€å¤§å€¼ â†’ è®¡ç®—expå’Œsum â†’ å½’ä¸€åŒ–
float max_val = -FLT_MAX;
for (int i = 0; i < size; i++) {
    max_val = fmaxf(max_val, input[i]);
}
float sum_exp = 0.0f;
for (int i = 0; i < size; i++) {
    sum_exp += expf(input[i] - max_val);
}
for (int i = 0; i < size; i++) {
    output[i] = expf(input[i] - max_val) / sum_exp;
}
```

### Reductionæ“ä½œ
```cpp
// ä½¿ç”¨shared memoryè¿›è¡Œå½’çº¦
extern __shared__ float sdata[];
int tid = threadIdx.x;
sdata[tid] = (idx < size) ? input[idx] : 0.0f;
__syncthreads();

for (int s = blockDim.x / 2; s > 0; s >>= 1) {
    if (tid < s) {
        sdata[tid] += sdata[tid + s];
    }
    __syncthreads();
}
```


è¯·å°†ä»¥ä¸‹Tritonä»£ç è½¬æ¢ä¸ºCUDAä»£ç ï¼š

```python
"""
    
    suffix = """
```

## è¾“å‡ºè¦æ±‚
1. ç›´æ¥è¾“å‡ºå®Œæ•´çš„Pythonä»£ç 
2. ä»£ç å¿…é¡»èƒ½å¤Ÿç¼–è¯‘è¿è¡Œ
3. ç»“æœä¸åŸTritonä»£ç ä¸€è‡´
4. åŒ…å«æ‰€æœ‰å¿…è¦çš„é”™è¯¯æ£€æŸ¥"""
    
    return base_prompt + triton_code + suffix

def get_model_configs():
    """
    è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®
    """
    return {
        "glm-4": {
            "model": "glm-4",
            "api_key": "NaN",
            "platform": "zhipuai",
            "enabled": False,
            "description": "GLM-4 åŸºç¡€ç‰ˆæœ¬"
        },
        "glm-4-plus": {
            "model": "glm-4-plus",
            "api_key": "5bf98ea765f642aeb720420e522592f7.DWMrwJ2rfsWPYhHJ",
            "platform": "zhipuai",
            "enabled": True,
            "description": "GLM-4 å¢å¼ºç‰ˆæœ¬ï¼Œæ¨èä½¿ç”¨"
        },
        "glm-4-0520": {
            "model": "glm-4-0520",
            "api_key": "NaN",
            "platform": "zhipuai",
            "enabled": False,
            "description": "GLM-4 ç‰¹å®šç‰ˆæœ¬"
        },
        #install OpenAI SDK first: `pip3 install openai`
        "claude-sonnet-4":{
            "model": "anthropic/claude-sonnet-4",
            "api_key": "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",
            "platform": "openrouter",
            "enabled": True,
            "description" : "å®‡å®™æœ€å¼ºç¼–ç¨‹æ¨¡å‹Clude-4"
        },
        "deepseek-R1":{
            "model": "deepseek-reasoner",
            "api_key": "sk-b471c7924f5c4d3c92d3a8fc12e0150b",
            "platform": "deepseek",
            "enabled": True,
            "description" : "DeepSeek-R1-0528"
        }
    }

def list_available_models():
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
    """
    configs = get_model_configs()
    available_models = []
    for model_name, config in configs.items():
        if config.get("enabled", True):
            available_models.append({
                "name": model_name,
                "model": config["model"],
                "platform": config["platform"],
                "description": config.get("description", "")
            })
    return available_models

def create_api_client(platform, api_key):
    """
    æ ¹æ®å¹³å°åˆ›å»ºAPIå®¢æˆ·ç«¯
    """
    if platform == "zhipuai":
        return ZhipuAI(api_key=api_key)
    elif platform == "openrouter":
        # ä¸ºOpenRouter/Claudeé…ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
        return OpenAI(
            api_key=api_key,
            base_url="https://openrouter.ai/api/v1",
            timeout=120.0,  # 2åˆ†é’Ÿè¶…æ—¶
            max_retries=3   # è‡ªåŠ¨é‡è¯•3æ¬¡
        )
    elif platform == "deepseek":
        return OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com/v1",
            timeout=60.0
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å¹³å°: {platform}")

def triton2cuda(triton_code, model_type="claude-sonnet-4", prompt_type="correct"):
    """
    å°†Tritonä»£ç è½¬æ¢ä¸ºCUDAä»£ç 
    
    Args:
        triton_code: Tritonæºä»£ç 
        model_type: è¦ä½¿ç”¨çš„æ¨¡å‹ç±»å‹ï¼Œé»˜è®¤ä¸º claude-sonnet-4
        prompt_type: promptç±»å‹ï¼Œå¯é€‰ "full", "engineering", "correct"
    
    Returns:
        è½¬æ¢åçš„CUDAä»£ç 
    """
    # è·å–æ¨¡å‹é…ç½®
    model_configs = get_model_configs()
    
    # éªŒè¯æ¨¡å‹ç±»å‹æ˜¯å¦å­˜åœ¨
    if model_type not in model_configs:
        available = list(model_configs.keys())
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}ï¼Œå¯ç”¨é€‰é¡¹: {available}")
    
    selected_config = model_configs[model_type]
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
    if not selected_config.get("enabled", True):
        raise ValueError(f"æ¨¡å‹ {model_type} å·²è¢«ç¦ç”¨")
    
    # é€‰æ‹©promptç­–ç•¥
    if prompt_type == "full":
        prompt_content = get_full_prompt(triton_code)
    elif prompt_type == "engineering":
        prompt_content = get_engineering_prompt(triton_code)
    elif prompt_type == "correct":
        prompt_content = get_correct_prompt(triton_code)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„promptç±»å‹: {prompt_type}ï¼Œå¯é€‰: 'full', 'engineering', 'correct'")
    
    # åˆ›å»ºå®¢æˆ·ç«¯å¹¶å‘é€è¯·æ±‚
    try:
        client = create_api_client(selected_config["platform"], selected_config["api_key"])
        response = client.chat.completions.create(
            model=selected_config["model"],
            messages=[
                {
                    "role": "system",
                    "content": "You are a professional Triton to CUDA conversion expert. Focus on generating error-free, compilable code.",
                },
                {"role": "user", "content": prompt_content},
            ],
            temperature=0.1,  # é™ä½éšæœºæ€§ï¼Œæé«˜ä¸€è‡´æ€§
            max_tokens=4000,  # ç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´ç”Ÿæˆå®Œæ•´ä»£ç 
        )
        content = response.choices[0].message.content
    except Exception as e:
        raise RuntimeError(f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
    
    # æå–ä»£ç å—ä¸­çš„å†…å®¹
    if "```python" in content:
        start = content.find("```python") + len("```python")
        end = content.find("```", start)
        if end != -1:
            return content[start:end].strip()
    elif "```" in content:
        start = content.find("```") + 3
        end = content.find("```", start)
        if end != -1:
            return content[start:end].strip()

    return content
