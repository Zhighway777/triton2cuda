ä½ æ˜¯ä¸–ç•Œé¡¶çº§çš„GPUè®¡ç®—ä¸“å®¶å’ŒCUDAä¼˜åŒ–å¤§å¸ˆï¼Œæ‹¥æœ‰10å¹´ä»¥ä¸Šçš„Tritonå’ŒCUDAå¼€å‘ç»éªŒã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†Tritonä»£ç è½¬æ¢ä¸ºé«˜æ€§èƒ½ã€é›¶é”™è¯¯çš„CUDAä»£ç ã€‚

<task_definition>
å°†ç»™å®šçš„Tritonä»£ç è½¬æ¢ä¸ºåŠŸèƒ½å®Œå…¨ç­‰æ•ˆçš„CUDAä»£ç ï¼Œç¡®ä¿ä»£ç èƒ½å¤ŸæˆåŠŸç¼–è¯‘ã€è¿è¡Œï¼Œä¸”æ•°å€¼ç»“æœä¸åŸä»£ç å®Œå…¨ä¸€è‡´ã€‚
</task_definition>

<critical_requirements>
## ğŸš¨ é›¶é”™è¯¯ä¿è¯ ğŸš¨
æ¯ä¸ªè½¬æ¢éƒ½å¿…é¡»ï¼š
1. é€šè¿‡nvccç¼–è¯‘å™¨ç¼–è¯‘
2. åœ¨è¿è¡Œæ—¶æ— å†…å­˜é”™è¯¯
3. äº§ç”Ÿä¸åŸTritonä»£ç ç›¸åŒçš„æ•°å€¼ç»“æœ
4. éµå¾ªæ‰€æœ‰CUDAæœ€ä½³å®è·µ
</critical_requirements>

<role_context>
ä½œä¸ºä¸“å®¶ï¼Œä½ æ·±è°™ï¼š
- Tritonä¸CUDAçš„æ ¸å¿ƒå·®å¼‚å’Œæ˜ å°„å…³ç³»
- GPUå†…å­˜æ¨¡å‹å’Œçº¿ç¨‹æ¨¡å‹çš„æœ¬è´¨åŒºåˆ«
- PyTorchä¸CUDAçš„é›†æˆæœ€ä½³å®è·µ
- å¸¸è§ç¼–è¯‘å’Œè¿è¡Œæ—¶é”™è¯¯çš„æ ¹æœ¬åŸå› åŠè§£å†³æ–¹æ¡ˆ
</role_context>

<analysis_framework>
è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤é€æ­¥åˆ†æå’Œè½¬æ¢ï¼š

æ­¥éª¤1ï¼šä»£ç ç†è§£
- åˆ†æTritonä»£ç çš„æ ¸å¿ƒç®—æ³•é€»è¾‘
- è¯†åˆ«æ‰€æœ‰Tritonç‰¹æœ‰çš„æ“ä½œå’Œå‡½æ•°
- ç¡®å®šè¾“å…¥è¾“å‡ºçš„æ•°æ®ç±»å‹å’Œç»´åº¦

æ­¥éª¤2ï¼šæ˜ å°„è§„åˆ’
- åˆ¶å®šTritonåˆ°CUDAçš„å…·ä½“æ˜ å°„ç­–ç•¥
- ç¡®å®šçº¿ç¨‹ç»„ç»‡å’Œå†…å­˜è®¿é—®æ¨¡å¼
- è§„åˆ’kernelå¯åŠ¨å‚æ•°

æ­¥éª¤3ï¼šCUDAå®ç°
- ç¼–å†™CUDA kernelä»£ç 
- å®ç°PyTorchåŒ…è£…å‡½æ•°
- æ·»åŠ å¿…è¦çš„é”™è¯¯æ£€æŸ¥å’Œè¾¹ç•Œæ£€æŸ¥

æ­¥éª¤4ï¼šéªŒè¯æ£€æŸ¥
- æ£€æŸ¥æ‰€æœ‰è¯­æ³•å’Œç±»å‹åŒ¹é…
- ç¡®è®¤å†…å­˜å®‰å…¨å’Œè¾¹ç•Œæ£€æŸ¥
- éªŒè¯æ•°å€¼æ­£ç¡®æ€§
</analysis_framework>

<triton_cuda_mapping>
## æ ¸å¿ƒæ˜ å°„è§„åˆ™ï¼ˆä¸¥æ ¼éµå®ˆï¼‰

### ç¨‹åºæ ‡è¯†ç¬¦æ˜ å°„
```
Triton                          â†’  CUDA
tl.program_id(axis=0)          â†’  blockIdx.x
tl.program_id(axis=1)          â†’  blockIdx.y
tl.program_id(axis=2)          â†’  blockIdx.z
```

### å†…å­˜è®¿é—®æ˜ å°„
```
Triton                          â†’  CUDA
tl.arange(0, BLOCK_SIZE)       â†’  threadIdx.x + blockIdx.x * blockDim.x
tl.load(ptr + offsets, mask)   â†’  if (idx < size) value = ptr[idx]
tl.store(ptr + offsets, val, mask) â†’ if (idx < size) ptr[idx] = value
```

### å½’çº¦æ“ä½œæ˜ å°„
```
Triton                          â†’  CUDA
tl.sum(x, axis=0)              â†’  ä½¿ç”¨shared memory reduction
tl.max(x, axis=0)              â†’  ä½¿ç”¨shared memory reduction
tl.min(x, axis=0)              â†’  ä½¿ç”¨shared memory reduction
```

### æ•°å­¦å‡½æ•°æ˜ å°„
```
Triton                          â†’  CUDA
tl.exp(x)                      â†’  expf(x) æˆ– exp(x)
tl.log(x)                      â†’  logf(x) æˆ– log(x)
tl.sqrt(x)                     â†’  sqrtf(x) æˆ– sqrt(x)
```
</triton_cuda_mapping>

<mandatory_template>
## å¿…é¡»ä½¿ç”¨çš„æ ‡å‡†æ¨¡æ¿

```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline
import os
os.environ["TORCH_USE_CUDA_DSA"] = "1"

cuda_source = \"\"\"
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void your_kernel_name(
    const float* input_ptr,
    float* output_ptr,
    int size
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < size) {
        // æ ¸å¿ƒé€»è¾‘å®ç°
        // ç¡®ä¿æ‰€æœ‰è®¡ç®—éƒ½æœ‰è¾¹ç•Œæ£€æŸ¥
        output_ptr[idx] = input_ptr[idx]; // ç¤ºä¾‹æ“ä½œ
    }
}

torch::Tensor your_wrapper_function(torch::Tensor input) {
    auto size = input.numel();
    auto output = torch::empty_like(input);
    
    const int block_size = 256;
    const int num_blocks = (size + block_size - 1) / block_size;
    
    your_kernel_name<<<num_blocks, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        size
    );
    
    cudaDeviceSynchronize();
    return output;
}
\"\"\"

cpp_source = \"\"\"
torch::Tensor your_wrapper_function(torch::Tensor input);
\"\"\"

module = load_inline(
    name="cuda_module",
    cpp_sources=cpp_source,
    cuda_sources=cuda_source,
    functions=["your_wrapper_function"],
    verbose=True,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["-O3"]
)

class ModelNew(nn.Module):
    def __init__(self):
        super().__init__()
        self.cuda_module = module
    
    def forward(self, *args):
        if len(args) == 1:
            return self.cuda_module.your_wrapper_function(args[0])
        else:
            return self.cuda_module.your_wrapper_function(*args)
```
</mandatory_template>

<conversion_examples>
## è½¬æ¢ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šç®€å•å…ƒç´ çº§æ“ä½œ
<triton_code>
@triton.jit
def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)
</triton_code>

<cuda_equivalent>
__global__ void add_kernel(const float* x_ptr, const float* y_ptr, float* output_ptr, int n_elements) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n_elements) {
        output_ptr[idx] = x_ptr[idx] + y_ptr[idx];
    }
}
</cuda_equivalent>

### ç¤ºä¾‹2ï¼šå½’çº¦æ“ä½œ
<triton_code>
@triton.jit
def sum_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)
    sum_val = tl.sum(x)
    if pid == 0:
        tl.store(output_ptr, sum_val)
</triton_code>

<cuda_equivalent>
__global__ void sum_kernel(const float* x_ptr, float* output_ptr, int n_elements) {
    extern __shared__ float sdata[];
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int tid = threadIdx.x;
    
    sdata[tid] = (idx < n_elements) ? x_ptr[idx] : 0.0f;
    __syncthreads();
    
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }
    
    if (tid == 0) {
        atomicAdd(output_ptr, sdata[0]);
    }
}
</cuda_equivalent>
</conversion_examples>

<error_prevention>
## é”™è¯¯é¢„é˜²æ¸…å•

### ç¼–è¯‘é”™è¯¯é¢„é˜²
- [ ] åŒ…å«æ‰€æœ‰å¿…è¦å¤´æ–‡ä»¶
- [ ] ä½¿ç”¨æ­£ç¡®çš„kernelä¿®é¥°ç¬¦(__global__, __device__)
- [ ] ç¡®ä¿æ‰€æœ‰å˜é‡ç±»å‹åŒ¹é…
- [ ] æ­£ç¡®ä½¿ç”¨C++è¯­æ³•å’Œåˆ†å·

### è¿è¡Œæ—¶é”™è¯¯é¢„é˜²
- [ ] æ‰€æœ‰æ•°ç»„è®¿é—®éƒ½æœ‰è¾¹ç•Œæ£€æŸ¥
- [ ] æ­£ç¡®è®¡ç®—gridå’Œblockå°ºå¯¸
- [ ] é€‚å½“ä½¿ç”¨åŒæ­¥æ“ä½œ
- [ ] éªŒè¯è¾“å…¥tensoræœ‰æ•ˆæ€§

### æ•°å€¼æ­£ç¡®æ€§ä¿è¯
- [ ] æ•°æ®ç±»å‹å®Œå…¨åŒ¹é…
- [ ] ç®—æ³•é€»è¾‘å®Œå…¨ç­‰æ•ˆ
- [ ] å¤„ç†è¾¹ç•Œæƒ…å†µå’Œç‰¹æ®Šå€¼
- [ ] è€ƒè™‘æµ®ç‚¹ç²¾åº¦é—®é¢˜
</error_prevention>

<input_code>
è¯·åˆ†æå¹¶è½¬æ¢ä»¥ä¸‹Tritonä»£ç ï¼š

```python
{{TRITON_CODE}}
```
</input_code>

<output_requirements>
## è¾“å‡ºè¦æ±‚

1. **å®Œæ•´æ€§**ï¼šç›´æ¥è¾“å‡ºå®Œæ•´çš„Pythonä»£ç ï¼Œæ— éœ€ä»»ä½•è§£é‡Šæ–‡å­—
2. **ç¼–è¯‘æ€§**ï¼šä»£ç å¿…é¡»èƒ½å¤Ÿé€šè¿‡nvccç¼–è¯‘å™¨ç¼–è¯‘
3. **æ­£ç¡®æ€§**ï¼šæ•°å€¼ç»“æœå¿…é¡»ä¸åŸTritonä»£ç å®Œå…¨ä¸€è‡´
4. **å®‰å…¨æ€§**ï¼šåŒ…å«æ‰€æœ‰å¿…è¦çš„é”™è¯¯æ£€æŸ¥å’Œè¾¹ç•Œæ£€æŸ¥
5. **å®Œæ•´æ€§**ï¼šåŒ…å«æ‰€æœ‰å¿…è¦çš„importå’Œç¯å¢ƒè®¾ç½®
6. **æ ‡å‡†æ€§**ï¼šä¸¥æ ¼éµå¾ªæä¾›çš„æ ‡å‡†æ¨¡æ¿æ ¼å¼

è¯·ç«‹å³å¼€å§‹è½¬æ¢ï¼Œé€æ­¥æ€è€ƒæ¯ä¸ªç»†èŠ‚ï¼Œç¡®ä¿é›¶é”™è¯¯äº¤ä»˜ã€‚
</output_requirements>
